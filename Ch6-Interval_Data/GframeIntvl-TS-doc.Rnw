\documentclass[english,11pt,oneside]{article}
\usepackage[T1]{fontenc}
\usepackage[left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm]{geometry}

\usepackage{ZRXmacrosYifan}

\usepackage[authoryear]{natbib}

\numberwithin{figure}{section}
\numberwithin{equation}{section}

\theoremstyle{plain}
      \newtheorem{thm}{Theorem}[section] %important theory
      \newtheorem{cor}{Corollary}[thm] %after thm
      \newtheorem{lem}[thm]{Lemma}
      \newtheorem{prop}[thm]{Proposition} %auxiliary theory
   
\theoremstyle{definition}
      \newtheorem{defn}[thm]{Definition}
      \newtheorem{assump}{Assumption}[section]     
      \newtheorem{eg}[thm]{Example}
      
\theoremstyle{remark}
      \newtheorem{re}{Remark}[thm]% including math expression 
      \newtheorem{com}{Comment}[thm] %only interpretation
      \newtheorem{que}{Question}[thm] %for asking questions
 
\makeatletter%what is the usage? 

\makeatother

\begin{document}

\title{Discussion Notes on the Interval-valued Time Series}

\author{Yifan Li}

\maketitle

This document is mainly written to be discussed with Prof.~Kulperger and Prof.~Yu. This is also highly related to our joint work with Prof.~Defei Zhang in the time series in the $G$-expectation framework. 

\setkeys{Gin}{height=0.6\linewidth}

<<include=FALSE>>=
knitr::opts_chunk$set(echo = FALSE)
@

<<>>=
source("GframeIntvl.R")
@


<<>>=
#generate the data
n <- 1e3
set.seed(123)
sd.intvl.true <- c(1,2)
w <- rsemiGnorm.intvl(n, sd.intvl = c(1,2))
w.obs <- as.propintvl(w)
mu0 <- 1
#mu0 <- rconstant.intvl(n, c(0,2))
x <- mu0 + w
x.obs <- as.propintvl(x)
@

\tableofcontents

\newpage 

\section{Introduction (Brief Version)}

We intend to start from a simplified construction on the interval
data which is highly related to the interval-valued time series data.
Since it is a simplified one, this construction would be essentially
connected with but different from our previous construction. It will
eventually lead us to an more intuitive interpretation of the sequential
independence in the $G$-expectation framework (in terms of the ambiguity
in the interval direction). 

Reference notes in the existing literature on interval-valued time series: (brief version to show our concerns)
%after more reading

\begin{enumerate}
\item Methods based on Symbolic Data Analysis (SDA), one typical example is to model the center and range process using standard ARMA model (ref: 2008-Luis), 
one concern here is the intervals here are all proper ones, so the range process will always be strictly non-negative,  a standard ARMA (without any transformations) may not be appropriate.

\item Methods originated from the Theory of Random Sets, 
One key concept here is the Kernel distance between intervals (different choice of kernels will significantly affect the interpretation of the distinction of intervals.)
One example in (ref:2013-Han) is a AR conditional Interval model, where they allows the improper intervals, but the parameters in the intervals are all point-valued. One of our focused points here is the parameter uncertainty which is characterized by the interval-valued parameter. 

\item In short, we intend to extend the existing design of  interval-valued time series to a study of considering the direction of the observed intervals and also allowing the parameter to be interval-valued.

\end{enumerate}

\section{Interval Operations}

Mainly recall the left and right scaling. 

\section{Theoretical Construction}

\subsection{Interval-valued Time Series: Simplified Version}

In practice, we can observe the interval-valued time series $\tilde{\intvl{X}_{t}},t=0,1,2,\dotsc$
which are almost always proper ones. It can be generally treated as
\[
\tilde{\intvl{X}}_{t}=1\intvl{X}_{t},
\]
for one specified model $\intvl{X}_{t}$ which may or may not involve
improper intervals. As a reminder, $1\intvl{X}_{t}$ is a left scaling
of the interval: $1[a,b]\coloneqq[a\wedge b,a\vee b]$ and it is also
the ensured proper transformation.

One straightforward candidate model is $\intvl{X}_{t}\coloneqq\tilde{\intvl{X}}_{t}$,
that only considers proper intervals which are the observed ones. 

To be specific, 
\begin{align*}
\tilde{\intvl{X}_{t}} & =1\intvl{X}_{t}=1[\myl{\intvl{X}_{t}},\myr{\intvl{X}_{t}}]\\
 & =[\myl{\intvl{X}_{t}}\wedge\myr{\intvl{X}_{t}},\myl{\intvl{X}_{t}}\vee\myr{\intvl{X}_{t}}].
\end{align*}
In terms of the center and range of the intervals: 
\begin{equation}
\intvlctr{\tilde{\intvl{X}}_{t}}=\intvlctr{\intvl{X}_{t}}=\frac{1}{2}(\myl{\intvl{X}_{t}}+\myr{\intvl{X}_{t}}),\label{eq:intvlctr-rel}
\end{equation}
and 
\begin{equation}
\intvlrng{\tilde{\intvl{X}}_{t}}=\abs{\intvlrng{\intvl{X}_{t}}}=\abs{\myr{\intvl{X}_{t}}-\myl{\intvl{X}_{t}}}.\label{eq:intvlrng-rel}
\end{equation}
The relation shown in \ref{eq:intvlctr-rel} tells us the center of
the observed interval will be the same as the center of the underlying
model no matter how the underlying model is specified (which is unknown
from the side of data analysts). 

For simplicity, in this context, for a specified model 
\[
\intvl{X}_{t}=[\myl{\intvl{X}_{t}},\myr{\intvl{X}_{t}}],
\]
 where each end can be treated as a stochastic process in the classical
$(\Omega,\sigmafield{F},(\sigmafield{F}_{t})_{t\geq0},\lprob)$. For
each time $t$, let us define its interval-valued expectation as 
\[
\intvlexpt\intvl{X}_{t}\coloneqq[\lexpt[\myl{\intvl{X}_{t}}],\lexpt[\myr{\intvl{X}_{t}}]].
\]
If $\intvl{X}_{t}$ is degenerate (i.e. $\myl{\intvl{X}_{t}}\equiv\myr{\intvl{X}_{t}}$),
the interval-valued expectation $\intvlexpt$ is degenerated into
the linear expectation $\lexpt$. 

(Later on, when we consider the ambiguity in the specification of
$\intvl{X}_{t}$ or the interval direction, the the interval supreme
of a class of $\intvlexpt$ will be become the $\hat{\intvlexpt}$
where the two ends become the sublinear expectation. This is especially
useful in the study of the observed $\tilde{\intvl{X}}_{t}$ where
we are not sure of the underlying interval direction.) 

\section{A Series of Examples with Simulation }

\subsection{Mean Certainty and Variance Uncertainty }

\subsubsection{Data Problem Setup}

Imagine a data situation as follows: we want to study the property
of a time series $X_{t}$ which is known to be a strong stationary
one with $\lexpt[|X_{t}|^{3}]<\infty$ (for simplicity of this problem).
Let $\mu\coloneqq\lexpt[X_{t}]$ and $\sigma^{2}\coloneqq\lexpt[(X_{t}-\mu)^{2}]$. 

Suppose we ask a lab to help us do the measurement. The lab tells
us they have two (secret) equipments,labeled \textbf{A} and \textbf{B},
as measurers and they have measured the $X_{t}$ for $t=1,2,\cdots,n$
using both of them. At each time $t$, Equipment \textbf{A} will get
the measurement $X_{tl}$ and Equipment \textbf{B }gives $X_{tr}$. 

We are also informed from the lab that: 
\begin{enumerate}
\item Based on the design of these two equipments and measurement errors,
at each time $t$, the true $X_{t}$ must be between $X_{tl}$ and
$X_{tr}$.
\item Nonetheless, we are told that both of them are valid equipments, in
the sense that $X_{tl}$ and $X_{tr}$ are precise in the mean part
of $X_{t}$ but may not be precise in the variance part of $X_{t}$. 
\end{enumerate}
However, due to some reasons (e.g. privacy problem), the lab refuses
to give us the measurements $\intvl{X}_{t}=(X_{tl},X_{tr})$ labelled
with \textbf{A} and \textbf{B} (it may contain some privacy information
of the two equipments), the lab only provides us with the minimum
and maximum measurements at each time (which is $\tilde{\intvl{X}}_{t}=1\intvl{X}_{t}$).

In short, we only observe $\tilde{\intvl{X}}_{t}=1\intvl{X}_{t}$.
Based on the information we may assume the two unobserved objects
$\intvl{X}_{t}$ and $X_{t}$ as follows:
\begin{enumerate}
\item For each $t$, $X_{t}\in1\intvl{X}_{t}=\tilde{\intvl{X}_{t}},$ 
\item $\intvlexpt\intvl{X}_{t}=[\mu,\mu]=\mu\coloneqq\lexpt[X_{t}],$ 
\item $\lexpt[(X_{t}-\mu)^{2}]\eqqcolon\sigma^{2}\in1\intvlexpt(\intvl{X}_{t}-\mu)^{2}\eqqcolon[\sdl^{2},\sdr^{2}]$
with $0<\sdl\leq\sdr$ (For simplicity, we assume the variance of
the either ends of $\intvl{X}_{t}$ only switch betwen $\sdl^{2}$
and $\sdr^{2}$. 
\end{enumerate}
We are interested in two specific problems: 
\begin{enumerate}
\item (P1) Esimation of parameters of $\intvl{X}_{t}$: based on data $\tilde{\intvl{X}_{t}}$,
we want to estimate the parameter $(\mu,\sdl,\sdr)$ of $\intvl{X}_{t}$
and also construct a reasonable confidence interval for $\mu$ . 
\item (P2) Study of the normalized sum of $X_{t}$: if we are able to estimate
$(\mu,\sdl,\sdr)$, for $\varphi(x)=x^{3}$ (or other monotone functions),
suppose we treat the estimated values as the true ones (if they are
precise enough), let 
\[
Y_{n}=\frac{1}{\sqrt{n}}\sum_{t=1}^{n}(X_{t}-\mu),
\]
and 
\[
\intvl{Y}_{n}=\frac{1}{\sqrt{n}}\sum_{t=1}^{n}(\intvl{X}_{t}-\mu).
\]
Since$Y_{n}$must between the two ends of $\intvl{Y}_{n}$, we have
$\lexpt[\varphi(Y_{n})]$ must be covered by 
\[
[\lexpt[\varphi(\myl{\intvl{Y}_{n}})],\lexpt[\varphi(\myr{\intvl{Y}_{n}})]].
\]
We want to study the possibilities of this envelope. 
\end{enumerate}
Let us forcus on (P1) first. One thing we can think about: does the
lackness of information in interval direction cause a problem in the
estimation of $(\mu,\sdl,\sdr)$? (Intuitively, does the loss of interval
direction mask information on the parameters of $\intvl{X}_{t}$?)

\subsubsection{Initial Analysis}

We first need to do some initial analysis on the observed interval
time series. 

Here are the plots of the observed interval data (first 100 intervals). 

<<fig=TRUE,out.width='0.8\\linewidth'>>=
sub.ind <- seq_len(100)
#plot.intvl(w[sub.ind,])
#plot.intvl(w.obs[sub.ind,])
#plot.intvl(x[sub.ind,])
plot.intvl(x.obs[sub.ind,])

plot.intvl(x.obs[sub.ind, ], linesplot = FALSE, fatten = 0)

#how to do the adjustment to do the estimation
@


\ref{eq:intvlctr-rel} shows the observed interval has
the same center as the true one, so we can use this property to at
least check the distributional information. 

<<fig=TRUE,out.width='0.8\\linewidth'>>=
x.obs.cen <- apply(x.obs, 1, center)
qqnorm(x.obs.cen, main = "Normal Q-Q Plot of the Center of X.obs")
qqline(x.obs.cen, col=2) #it seems almost like a normal
@

We can see that the center sequence of the intervals is approximately
normally distributed. 

However, if we can check the distributions of the two ends. 

<<fig=TRUE,out.width='0.8\\linewidth'>>=
hist(x.obs[,1], main = "Histogram of the Left End of X.obs")
qqnorm(x.obs[,1], main = "Normal Q-Q Plot of the Left End of X.obs")
qqline(x.obs[,1], col=2)

hist(x.obs[,2], main = "Histogram of the Right End of X.obs")
qqnorm(x.obs[,2], main = "Normal Q-Q Plot of the Right End of X.obs")
qqline(x.obs[,2], col=2)
@

They seems not normally distributed at least from the skewness. 

%naively use the two ends
%If we naively use the sample mean two ends of the observed intervals, it will lead to seriously misleading results because what we interested in is the 

If we naively use the sample mean of the two ends of the observed intervals, it will lead to misleading results because we already know that the true mean should be a single value (or a degenerate interval). However, the sample mean of the two ends of the observed intervals will gives us a non-degenerate interval and this interval will never shrink to a single value as sample size increase.

This is also one of the distinctions of our work from part of the existing literature based on the LLN from Aumann expectation where the sample mean is directly applied to the observed intervals itself: it definitely a feasible method but it essentially treats the observed intervals as the true underlying process and the expectation will be the limit of the observed sample mean: it does not always the case, especially when we involves the direction of the intervals.

We also want readers to not overinterpret this non-degenerate interval too much (it may cover the true $\mu$, but it has nothing to do with the confidence interval at any significance level.)
We will see this from the study of the distribution of the two ends. 

Our focus here is often the expectation (which should match the background of the dataset) which may or may not be the limit of the sample mean of the observed interval when the true underlying intervals are allowed to be improper ones. 
For instance, when we look at the interval-valued log return of a stock price, one may ask, what is the best expected return and the worst expected return we can get from the stock using some \emph{realistic} trading strategy? One important note of the realistic trading strategy is based on the  assumption that we cannot see into the future: it is unrealistic to ask one to always do the trading at the exact min and max point of the stock intraday price at each day accordingly to achieve the max or min daily return, but we can only do the trading based on some random rule and use the information up until the time point now to make the movement at the next time point (It is related to the predictability of the process). Therefore, if we simply use the sample mean of the upper and lower end of the observed log return, it will converge to an interval of expectations whose two ends are \emph{impossible} to achieve based on some realistic trading strategies. Therefore, a more appropriate min and max expected return should forms a smaller interval, which is the focus of our work. 



<<>>=
x.obs.cummean <- apply(x.obs,2,cummean)
plot.intvl(x.obs.cummean)
@


\subsubsection{The First Candidate Model}

One candidate model for the underlying true interval $\intvl{X}_{t}$
of $\tilde{\intvl{X}}_{t}$ is 
\begin{equation}
\intvl{X}_{t}^{(1)}=\mu+[\sdl,\sdr]\epsilon_{t},\label{eq:mean-certainty-model1}
\end{equation}
and then $\tilde{\intvl{X}}_{t}=1\intvl{X}_{t}^{(1)}$. In the following
context of this section, since we will mainly focus on $\intvl{X}_{t}^{(1)}$,
without causing any confusions, we will omit the superscript and write
it as $\intvl{X}_{t}$ but we want readers to know that it is only
a candidate \textbf{model} for the true underlying interval process
but not the true process itself. 

In \ref{eq:mean-certainty-model1}, $\epsilon_{t}$ are classically
i.i.d. $\CN(0,1)$ (or any other white noise process with $\lexpt[\epsilon_{t}]=0$
and $\lexpt[\epsilon_{t}^{2}]=1$). and the operation in $[\sdl,\sdr]\epsilon_{t}$
is the right scaling ($[\myl{x},\myr{x}]c\coloneqq[\myl{x}c,\myr{x}c])$.
We also require $0<\sdl\leq\sdr$ for the identifiability of model
(because we will consider $\tilde{\intvl{X}}_{t}$ later on). When
$\sdl=\sdr=\sigma$, $\intvl{X}_{t}$ is degenerated into the shifted
Gaussian white noise $\intvl{X}_{t}\sim N(\mu,\sigma^{2})$. 


\subsubsection{Properties of the First Model}

Under the model specified by \ref{eq:mean-certainty-model1} by assuming
$\epsilon_{t}\sim\CN(0,1)$ in terms of the two ends, we can represent the true underlying process
$\intvl{X}_{t}$, which cannot be directly observed, as (we may call it the two-end representation) 
\[
\begin{cases}
\myl{\intvl{X}_{t}}=\mu+\sdl\epsilon_{t} & \text{i.i.d.}\sim\CN(\mu,\sdl^{2})\\
\myr{\intvl{X}_{t}}=\mu+\sdl\epsilon_{t} & \text{i.i.d.}\sim\CN(\mu,\sdr^{2})
\end{cases}.
\]
The center and range of $\intvl{X}_{t}$ are (we may call it the center-range representation), 
\[
\begin{cases}
\intvlctr{\intvl{X}_{t}}=\mu+\frac{\sdl+\sdr}{2}\epsilon_{t} & \text{i.i.d.}\sim\CN(\mu,\frac{(\sdl+\sdr)^{2}}{4})\\
\intvlrng{\intvl{X}_{t}}=(\sdr-\sdl)\epsilon_{t} & \text{i.i.d.}\sim\CN(0,(\sdr-\sdl)^{2})
\end{cases}.
\]
Under this model specification, we also have the properties that 
\begin{align*}
\intvlexpt\intvl{X}_{t} & =[\mu,\mu],\\
\intvlexpt\intvl{X}_{t}^{2} & =[\mu^{2}+\sdl^{2},\mu^{2}+\sdr^{2}],\\
\intvlexpt\intvl{X}_{t}^{3} & =[0,0],\\
\intvlexpt\ind{\intvl{X}_{t}<c} & =[\Phi(\frac{c-\mu}{\sdl}),\Phi(\frac{c-\mu}{\sdr})].
\end{align*}
and in terms of the sample mean 
\[
\bar{\intvl{X}}_{n}=\frac{1}{n}\sum_{t=1}^{n}\intvl{X}_{t},
\]
in terms of the two ends, we have 
\[
\bar{\intvl{X}}_{n}\convergeto{\text{a.s.}}\intvlexpt\intvl{X}_{t}=\intvl{\mu}=[\mu_{l},\mu_{r}]=[\mu,\mu],
\]
and 
\begin{align*}
\sqrt{n}(\bar{\intvl{X}}_{n}-\intvl{\mu}) & =\frac{1}{\sqrt{n}}\sum_{t=1}^{n}[\sdl,\sdr]\epsilon_{t}\\
 & =[\sdl,\sdr]\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\epsilon_{t}\\
 & \convergeto{\text{d}}[\sdl,\sdr]Z,
\end{align*}
where $Z\sim N(0,1)$. 

If we have the data directly for $\intvl{X}_{t}$, it is straightforward
to use traditional statistical estimation methods (such as maximum
likelihood estimation and method of moments) to make inference on
$(\mu,\sdl,\sdr)$. However, since we can only observe 
\begin{equation}
\tilde{\intvl{X}}_{t}=1\intvl{X}_{t}=[\myl{\intvl{X}_{t}}\wedge\myr{\intvl{X}_{t}},\myl{\intvl{X}_{t}}\vee\myr{\intvl{X}_{t}}],\label{eq:obs-intvl-rel1}
\end{equation}
The problem here is whether there still exists a tractable method
for us to still get reasonable estimation on $(\mu,\sdl,\sdr)$ from
the data $\tilde{\intvl{X}}_{t}$? We will give a positive answer
in this section. 

However, we only have $\tilde{\intvl{X}}_{t}$ observed and our goal
is to the estimation for the parameters $(\mu,\sdl,\sdr)$ in $\intvl{X}_{t}$
which is are directly observed. Here are two possible directions: 
\begin{enumerate}
\item Derive the distributions of the two ends of $\tilde{\intvl{X}}_{t}$
based on the model of $\intvl{X}_{t}$, then use traditional statistical
methods (such as MLE, MM or estimating equations) to estimate the
parameters;
\item Adjust the direction of the observed data $\tilde{\intvl{X}}_{t}$
into $\intvl{X}_{t}'$ to make it close to the model of $\intvl{X}_{t}$,
then treat $\intvl{X}_{t}'$ as $\intvl{X}_{t}$ to directly use the
distributions of $\intvl{X}_{t}$ to do the estimation. 
\end{enumerate}

\subsubsection{Distributions of the observed data}

Specifically,
given the distributions of $\myl{\intvl{X}_{t}}$ and $\myr{\intvl{X}_{t}}$,
from \ref{eq:obs-intvl-rel1}, we can derive the distributions of
the two ends of $\tilde{\intvl{X}}_{t}$: with $x^{+}\coloneqq\max\{x,0\}$
and $x^{-}\coloneqq\min\{-x,0\}$, 
\[
\myl{\tilde{\intvl{X}_{t}}}=\begin{cases}
\mu+\sdl\epsilon_{t} & \text{if }\epsilon_{t}\geq0\\
\mu+\sdr\epsilon_{t} & \text{if }\epsilon_{t}<0
\end{cases}=\mu+(\sdl\epsilon_{t}^{+}-\sdr\epsilon_{t}^{-}),
\]
and similarly, 
\[
\myr{\tilde{\intvl{X}_{t}}}=\mu+(\sdr\epsilon_{t}^{+}-\sdl\epsilon_{t}^{-}).
\]


We can first work on a simplified case: for $\epsilon\sim N(0,1)$
and $0<\sdl\leq\sdr$, consider $Y_{r}=(\sdl\epsilon)\vee(\sdr\epsilon)=\sdr\epsilon^{+}-\sdl\epsilon^{-}$
which will switch between $\sdr\epsilon$ and $\sdl\epsilon$ depending
on the sign of $\epsilon$. 

Let $\Phi$ and $\phi$ be the cdf and pdf of $\CN(0,1)$, respectively.
Then we can write the cdf of $Y_{r}$ as 
\begin{align*}
F_{Y_{r}}(y) & =\lprob(Y_{r}\leq y)\\
 & =\lprob(Y_{r}\leq y,\epsilon<0)+\lprob(Y_{r}\leq y,\epsilon>0)\\
 & =\lprob(\sdl\epsilon\leq y,\epsilon<0)+\lprob(\sdr\epsilon\leq y,\epsilon>0).\\
 & =\begin{cases}
0+\lprob(\epsilon\leq y/\sdl,\epsilon<0)=\lprob(\epsilon<y/\sdl) & \text{if }y<0\\
\lprob(\epsilon<0)+\lprob(0<\epsilon\leq y/\sdr)=\lprob(\epsilon\leq y/\sdr) & \text{if }y\geq0
\end{cases}\\
 & =\begin{cases}
\Phi(y/\sdl) & \text{if }y<0\\
\Phi(y/\sdr) & \text{if }y\geq0
\end{cases}.
\end{align*}

For $Z_{r}=\mu+Y_{r}$, we have 
\begin{align*}
F_{Z_{r}}(z) & =\lprob(Z_{r}\leq z)=\lprob(Y_{r}\leq z-\mu)=F_{Y_{r}}(z-\mu).
\end{align*}
Therefore, 
\[
F_{Z_{r}}(z)=\begin{cases}
\Phi(\frac{z-\mu}{\sdl}) & \text{if }z<\mu\\
\Phi(\frac{z-\mu}{\sdr}) & \text{if }z\geq\mu
\end{cases}=\begin{cases}
\Phi(\frac{z-\mu}{\sdl}) & \text{if }z<\mu\\
\frac{1}{2} & \text{if }z=\mu\\
\Phi(\frac{z-\mu}{\sdr}) & \text{if }z>\mu
\end{cases}.
\]
Note that $F_{Z_{r}}(z)$ is not differentiable at $z=\mu$, but it
has a continuous density function defined on $\numset{R}/\{\mu\}$,
\[
f_{Z_{r}}(z)=\frac{1}{\sdl}\phi(\frac{z-\mu}{\sdl})\ind{z<\mu}+\frac{1}{\sdr}\phi(\frac{z-\mu}{\sdr})\ind{z>\mu}.
\]
Similarly, the cdf and pdf $Z_{l}=\mu+Y_{l}=\mu+(\sdl\epsilon^{+}-\sdr\epsilon^{-})$
are, respectively, 
\[
F_{Z_{l}}(z)=\lprob(Z_{l}\leq z)=\begin{cases}
\Phi(\frac{z-\mu}{\sdr}) & \text{if }z<\mu\\
\Phi(\frac{z-\mu}{\sdl}) & \text{if }z\geq\mu
\end{cases},
\]
and 
\[
f_{Z_{l}}(z)=\frac{1}{\sdr}\phi(\frac{z-\mu}{\sdr})\ind{z<\mu}+\frac{1}{\sdl}\phi(\frac{z-\mu}{\sdl})\ind{z>\mu}.
\]
Note that $\myr{\tilde{\intvl{X}_{t}}}\eqdistn Z_{r}$ and $\myl{\tilde{\intvl{X}_{t}}}\eqdistn Z_{l}$
for each $t$.

Then we can validate our results by comparing with simulation. 

<<>>=
#emp density vs pdf
x.seq <- seq(-5, 10, .01)
ylim1 <- c(0,0.4)
#hist(x.obs[,1], probability = TRUE, ylim = ylim1, breaks = "scott")
hist(x.obs[,1], probability = TRUE, ylim = ylim1)
#lines(density(x.obs[,1]), col=2)
lines(x.seq, dnorm.intvll(x.seq), type = "l", col=2)

#hist(x.obs[,2], probability = TRUE, ylim = ylim1, breaks = "scott")
hist(x.obs[,2], probability = TRUE, ylim = ylim1)
#lines(density(x.obs[,2]), col=2)
lines(x.seq, dnorm.intvlr(x.seq), type = "l", col=2)

#ecdf vs cdf
plot(ecdf(x.obs[,1]))
lines(x.seq, pnorm.intvll(x.seq), type = "l", col=2)

plot(ecdf(x.obs[,2]))
lines(x.seq, pnorm.intvlr(x.seq), type = "l", col=2)
@


We can also derive the distributions for the center and range of $\tilde{\intvl{X}}_{t}$:
\[
\intvlctr{\tilde{\intvl{X}}_{t}}=\intvlctr{\tilde{\intvl{X}}_{t}}=\mu+\frac{\sdl+\sdr}{2}\epsilon_{t}\text{i.i.d.}\sim\CN(\mu,\frac{(\sdl+\sdr)^{2}}{4}),
\]
and 
\[
\intvlrng{\tilde{\intvl{X}}_{t}}=\abs{\intvlrng{\intvl{X}_{t}}}=\abs{(\sdr-\sdl)\epsilon_{t}},
\]
which follows the half normal distribution with scale parameter equal
to $(\sdr-\sdl)$.

For curiosity, we can compare the distributions of two ends of the observed data with the lower and upper cdf of the $G$-normal distributions. 

\subsubsection{Comparison with the $G$-normal cdf}

Surprisingly, it seems there exists some similarity between the $G$-normal cdfs (upper and lower) with the two-ends of the observed intervals (left and right). Although so far it is not clear whether there exists any theoretical explanations on this similarity, it must be interesting if we find one. 

The upper cdf of $G$-normal is (ref: $G$-VaR)
\begin{align*}
\myupper{F}_{\GN}(x) & \coloneqq\upprob(X\leq x)\\
 & =\int_{-\infty}^{x}\frac{\sqrt{2}}{(\sdr+\sdl)\sqrt{\pi}}\bigl[e^{-x^{2}/(2\sdr^{2})}\ind{x\leq0}+e^{-x^{2}/(2\sdl^{2})}\ind{x>0}]\diff x\\
 & =\begin{cases}
\frac{2\sdr}{(\sdr+\sdl)}\int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}\sdr}e^{-x^{2}/(2\sdr^{2})}\diff x=\frac{2\sdr}{\sdr+\sdl}\Phi(\frac{x}{\sdr}) & \text{if }x\leq0\\
\frac{2\sdr}{(\sdr+\sdl)}\Phi(0)+\frac{2\sdl}{(\sdr+\sdl)}\int_{0}^{x}\frac{1}{\sqrt{2\pi}\sdl}e^{-x^{2}/(2\sdl^{2})} & \text{if }x>0
\end{cases}\\
 & =\frac{2\sdr}{\sdr+\sdl}\Phi(\frac{x}{\sdr})\ind{x\leq0}+(\frac{2\sdl}{\sdr+\sdl}\Phi(\frac{x}{\sdl})+\frac{\sdr-\sdl}{\sdr+\sdl})\ind{x>0}.
\end{align*}
Since 
\begin{align*}
\frac{2\sdl}{\sdr+\sdl}\Phi(\frac{x}{\sdl})+\frac{\sdr-\sdl}{\sdr+\sdl} & =\frac{2\sdl}{\sdr+\sdl}\Phi(\frac{x}{\sdl})+1-\frac{2\sdl}{\sdr+\sdl}\\
 & =1-\frac{2\sdl}{\sdr+\sdl}(1-\Phi(\frac{x}{\sdl}))\\
 & =1-\frac{2\sdl}{\sdr+\sdl}\Phi(-\frac{x}{\sdl}),
\end{align*}
We further simplify the upper cdf as 
\begin{align*}
\myupper{F}_{\GN}(x) & =\frac{2\sdr}{\sdr+\sdl}\Phi(\frac{x}{\sdr})\ind{x\leq0}+(1-\frac{2\sdl}{\sdr+\sdl}\Phi(-\frac{x}{\sdl}))\ind{x>0},
\end{align*}
which simply retrieves the result in the $G$-VaR paper. Accordingly, The upper pdf is 
\begin{align*}
\myupper{f}_{\GN}(x) & \coloneqq\frac{\diff}{\diff x}\myupper{F}_{\GN}(x)\\
 & =\frac{\sqrt{2}}{(\sdr+\sdl)\sqrt{\pi}}\bigl[e^{-x^{2}/(2\sdr^{2})}\ind{x\leq0}+e^{-x^{2}/(2\sdl^{2})}\ind{x>0}]\\
 & =\frac{1}{\sqrt{2\pi}}\frac{1}{(\sdl+\sdr)/2}\bigl[e^{-x^{2}/(2\sdr^{2})}\ind{x\leq0}+e^{-x^{2}/(2\sdl^{2})}\ind{x>0}\bigr].
\end{align*}

It seems the $G$-normal pdf is one way (perhaps also the only way)
to bind two half-parts of the normal densities together also maintain the form a continuous density. 

As comparison, the pdf of the left end of the observed interval is
(with $\mu=0$): without changing its Lebesgue integral, we let $f_{Z_{l}}(0)\coloneqq f_{Z_{l}}(0-)$,
\begin{align*}
f_{Z_{l}}(x) & =\frac{1}{\sdr}\phi(\frac{x}{\sdr})\ind{x\leq0}+\frac{1}{\sdl}\phi(\frac{x}{\sdl})\ind{x>0}.\\
 & =\frac{1}{\sqrt{2\pi}}\bigl[\frac{1}{\sdr}e^{-x^{2}/(2\sdr^{2})}\ind{x\leq0}+\frac{1}{\sdl}e^{-x^{2}/(2\sdl^{2})}\ind{x>0}\bigr].
\end{align*}

We can see that they are similar in terms of the likelihood ratio (by treating $\myupper{f}_{\GN}(x)$ as a classical pdf,) 
\[
\frac{\myupper{f}_{\GN}(x)}{f_{Z_{l}}(x)}=\begin{cases}
\frac{2\sdr}{\sdl+\sdr} & \text{if }x\leq0\\
\frac{2\sdl}{\sdl+\sdr} & \text{if }x>0
\end{cases}=\frac{2}{\sdl+\sdr}(\sdr\ind{x\leq0}+\sdl\ind{x>0}).
\]
It indicates that we can do a straightforward change of measure if
we simply want to transform the density from one to the other (by
treating the $G$-normal cdf as a classical cdf). We can also do the
rejection sampling to reject a proportion of samples from $f_{Z_{l}}(x)$ to get a sample from $\myupper{f}_{\GN}(x)$. 

However, one need to be careful in terms of interpretation here. When
we say an iid sample drawn from $\myupper{f}_{\GN}(x)$ and $\myupper{F}_{\GN}(x)$,
we actually treat these two functions as classical pdf or cdf: imagine
a random variable $Y$ in classical probability space with $f_{Y}=\myupper{f}_{\GN}$
and $F_{Y}=\myupper{F}_{\GN}$ and we draw i.i.d. sample $Y_{1},Y_{2},\dotsc,Y_{n}$
from $Y$. 

Nonetheless, these sample is not ``an classically i.i.d. sample from
the $G$-normal distribution'', which is not a well-defined statement,
because $G$-normal distribution stays in a sublinear expectation
space where the concept of independence is different. 

The classically i.i.d. sample do not have much connection with the
$G$-normal because 
\[
\lim_{n\to\infty}\frac{1}{n}\sum_{i=1}^{n}\varphi(X_{i})\neq\expt[\varphi(X_{i})],
\]
except for those $\varphi(x)=\ind{x\leq a}$, not even for $\varphi(x)=\ind{x>a}$
nor $\varphi(x)=\ind{a<x\leq b}$, not to mention $\varphi(x)=x^{n}$. 

When $\sdl=\sdr=\sigma$, they will both retrieve the classical normal
$\CN(0,\sigma^{2})$. 


Similarly, we can compare the right end with the lower cdf of the
$G$-normal distribution. We have shown before that, for $X\sim\GN(0,\varI)$,
since $X\eqdistn-X$, we can show that,
\begin{align*}
\expt[\ind{X\leq-x}] & =\expt[\ind{X\leq-x}]\\
 & =\expt[\ind{-X>x}]\\
 & =\expt[1-\ind{-X\leq x}]\\
 & =1+\expt[-\ind{-X\leq x}]\\
 & =1+\expt[-\ind{X\leq x}]\\
 & =1-\lowprob[X\leq x].
\end{align*}
Let $\mylower{F}_{\GN}(x)\coloneqq\lowprob[X\leq x]$, then 
\[
\underline{F}_{\GN}(x)+\overline{F}_{\GN}(-x)=1.
\]

Hence, we have the lower cdf 
\begin{align*}
\underline{F}_{\GN}(x) & =1-\overline{F}_{\GN}(-x)\\
 & =\ind{x\geq0}+\ind{x<0}-\frac{2\sdr}{\sdr+\sdl}\Phi(\frac{-x}{\sdr})\ind{x\geq0}-(1-\frac{2\sdl}{\sdr+\sdl}\Phi(\frac{x}{\sdl}))\ind{x<0}\\
 & =\frac{2\sdl}{\sdr+\sdl}\Phi(\frac{x}{\sdl})\ind{x<0}+(1-\frac{2\sdr}{\sdr+\sdl}\Phi(\frac{-x}{\sdr}))\ind{x\geq0}.
\end{align*}
and the lower pdf, 
\begin{align*}
\mylower{f}_{\GN}(x) & =\frac{\diff}{\diff x}\mylower{F}_{\GN}(x)=\frac{\diff}{\diff x}(1-\myupper{F}_{\GN}(-x))\\
 & =\myupper{f}_{\GN}(-x)\\
 & =\frac{1}{\sqrt{2\pi}}\frac{1}{(\sdl+\sdr)/2}\bigl[e^{-x^{2}/(2\sdr^{2})}\ind{x\geq0}+e^{-x^{2}/(2\sdl^{2})}\ind{x<0}\bigr].
\end{align*}
Meanwhile, 
\begin{align*}
f_{Z_{r}}(x) & =\frac{1}{\sdr}\phi(\frac{x}{\sdr})\ind{x\geq0}+\frac{1}{\sdl}\phi(\frac{x}{\sdl})\ind{x<0}.\\
 & =\frac{1}{\sqrt{2\pi}}\bigl[\frac{1}{\sdr}e^{-x^{2}/(2\sdr^{2})}\ind{x\geq0}+\frac{1}{\sdl}e^{-x^{2}/(2\sdl^{2})}\ind{x<0}\bigr].
\end{align*}
Therefore, 
\[
\frac{\mylower{f}_{\GN}(x)}{f_{Z_{r}}(x)}=\begin{cases}
\frac{2\sdr}{\sdl+\sdr} & \text{if }x\geq0\\
\frac{2\sdl}{\sdl+\sdr} & \text{if }x<0
\end{cases}=\frac{2}{\sdl+\sdr}(\sdr\ind{x\geq0}+\sdl\ind{x<0}).
\]

Here is a comparison between the $\myupper{f}_{\GN}(x)$ and $f_{Z_{l}}(x)$.

<<>>=
sd.intvl.true <- c(1,2)
K <- 4*sd.intvl.true[2]
x.seq <- seq(-8,8,.01)
pnorm.intvl.seq <- dnorm.intvll(x.seq, mu=0, sd.intvl=sd.intvl.true)
pGnorm.seq <- dGnorm.upper(x.seq, sdl = sd.intvl.true[1], sdr = sd.intvl.true[2])
pnorm.cen <- dnorm(x.seq, sd = mean(sd.intvl.true))
#matplot(x.seq, cbind(pnorm.intvl.seq, pGnorm.seq, pnorm.cen), type = "l")

den.dat <- data.frame(x.seq=x.seq, 
                      den.Zl=pnorm.intvl.seq,
                      den.Gnorm.upper=pGnorm.seq)
den.dat2 <- melt(den.dat, id.var = "x.seq")
ggplot(den.dat2, aes(x=x.seq, y=value, colour=variable)) + geom_line() + scale_colour_manual(values=cbPalette)
@

We also have the comparison between the $\mylower{f}_{\GN}(x)$ and $f_{Z_{r}}(x)$.

<<>>=
sd.intvl.true <- c(1,2)
K <- 4*sd.intvl.true[2]
x.seq <- seq(-8,8,.01)
pnorm.intvl.seq <- dnorm.intvlr(x.seq, mu=0, sd.intvl=sd.intvl.true)
pGnorm.seq <- dGnorm.lower(x.seq, sdl = sd.intvl.true[1], sdr = sd.intvl.true[2])
#matplot(x.seq, cbind(pnorm.intvl.seq, pGnorm.seq), type = "l")
den.dat <- data.frame(x.seq=x.seq, 
                      den.Zr=pnorm.intvl.seq,
                      den.Gnorm.lower=pGnorm.seq)
den.dat2 <- melt(den.dat, id.var = "x.seq")
ggplot(den.dat2, aes(x=x.seq, y=value, colour=variable)) + geom_line() + scale_colour_manual(values=cbPalette)
@


\subsubsection{Sampling from G-normal cdf}

One question we may ask, since it is easy to sample from $Z_{l}$,
how can we modify the sample from $f_{Z_{l}}(x)$, under this likelihood
ratio, so that we can get a sample from $\myupper{f}_{\GN}(x)$? (So
far I do not think there is a simple way to do the sampling directly
from transforming the samples from $f_{Z_{l}}(x)$, except we use
inverse cdf, as shown in the ref:sim-G-BM or rejection sampling techniques.
)

If we use rejection sampling, let $g(x)=f_{Z_{l}}(x)$ and $f(x)=\myupper{f}_{\GN}(x)$,
our goal is to sample from $f(x)$ based on the sampling from $g(x)$.
(Try to see whether there is any simple forms coming out.) Since we
have the nice result that, 
\[
\frac{f(x)}{g(x)}=\frac{2}{\sdl+\sdr}(\sdr\ind{x\leq0}+\sdl\ind{x>0})\leq\frac{2\sdr}{\sdl+\sdr},
\]
we can choose 
\[
M\coloneqq\frac{2\sdr}{\sdl+\sdr}.
\]
Then we can perform the rejection sampling: 
\begin{enumerate}
\item Sample $X$ from $g(x)$, 
\item Generate $U$ from Unif(0,1), (independent from $X$) 
\item (R1) If 
\[
U\leq\frac{f(X)}{Mg(X)}=\frac{1}{\sdr}(\sdl\ind{X>0}+\sdr\ind{X\leq0})=\frac{\sdl}{\sdr}\ind{X>0}-\ind{X\leq0},
\]
set $X^{*}=X$ (``accept''); otherwise, go back to step 1 (``reject''). 
\item (or R2) If $X\leq0$, accept. If $X>0$ , generate $B$ from Bern($\sdl/\sdr)$,
accept $X$ if and only if $B=1$. 
\item (or R3) If $X\leq0$, accept. Accept the around first $\sdl/\sdr$
fraction of $X$ with $X>0$. 
\end{enumerate}
One note here, as long as the generated $X>0$ it will always accept.
The acceptance probability is 
\[
\frac{1}{M}=\frac{\sdl+\sdr}{2\sdr}.
\]

Accordingly, we can use the rejection sampling to sample from $f(x)=\mylower{f}_{\GN}(x)$ based on the sample from $g(x)=f_{Z_{r}}(x)$. (Try to see whether there is any simple forms coming out.) Since we have the nice result that, 
\[
\frac{f(x)}{g(x)}=\frac{2}{\sdl+\sdr}(\sdr\ind{x\geq0}+\sdl\ind{x<0})\leq\frac{2\sdr}{\sdl+\sdr},
\]
we can choose 
\[
M\coloneqq\frac{2\sdr}{\sdl+\sdr}.
\]
Then we can perform the rejection sampling: 
\begin{enumerate}
\item Sample $X$ from $g(x)$, 
\item Generate $U$ from Unif(0,1), (independent from $X$) 
\item If 
\[
U\leq\frac{f(X)}{Mg(X)}=\frac{1}{\sdr}(\sdr\ind{X\geq0}+\sdl\ind{X<0})=\frac{\sdl}{\sdr}\ind{X<0}+\ind{X\geq0},
\]
set $X^{*}=X$ (``accept''); otherwise, go back to step 1 (``reject''). 
\item (or) If $X\geq0$, accept. If $X<0$ , generate $B$ from Bern($\sdl/\sdr)$,
accept $X$ if and only if $B=1$.
\end{enumerate}

<<>>=
rintvl.norm.l <- function(n, sdl=1, sdr=2) {
  e <- rnorm(n)
  sdr*e*(e<=0)+sdl*e*(e>0)
}

rintvl.norm.r <- function(n, sdl=1, sdr=2) {
  e <- rnorm(n)
  sdr*e*(e>=0)+sdl*e*(e<0)
}
@

Then we can implement the rejection sampling. 

<<fig.width='0.6\\linewidth'>>=
#use R3

#try e and -e
e.seq <- rnorm(5e3)

rpGnorm.reject.intvl <- function(e.seq, sdl=1, sdr=2){
  M <- (sdl+sdr)/(2*sdr)
  #m <- n*M*2
  #e <- rnorm()
  e <- e.seq
  xl <- sdr*e*(e<=0)+sdl*e*(e>0) 
  xr <- sdl*e*(e<=0)+sdr*e*(e>0)
  keep1 <- which(xl<=0)
  keep2 <- which(xl>0)
  B.seq1 <- rbinom(length(keep2), 1, prob = sdl/sdr)
  B.seq2 <- rbinom(length(keep1), 1, prob = sdl/sdr)
  y.upper1 <- xl[c(keep1, keep2[B.seq1==1])]
  y.lower1 <- xr[c(keep2, keep1[B.seq2==1])]
  #l1 <- round(length(keep2)*(sdl/sdr))
  
  e <- -e.seq
  xl <- sdr*e*(e<=0)+sdl*e*(e>0) 
  xr <- sdl*e*(e<=0)+sdr*e*(e>0)
  keep1 <- which(xl<=0)
  keep2 <- which(xl>0)
  #B.seq1 <- rbinom(length(keep2), 1, prob = sdl/sdr)
  #B.seq2 <- rbinom(length(keep1), 1, prob = sdl/sdr)
  y.upper2 <- xl[c(keep1, keep2[B.seq2==1])]
  y.lower2 <- xr[c(keep2, keep1[B.seq1==1])]
  y.lower <- c(y.lower1, y.lower2)
  y.upper <- c(y.upper1, y.upper2)
  re <- cbind(y.lower, y.upper)
  re
}

re <- rpGnorm.reject.intvl(e.seq, 1,2)
x.seq <- seq(-6,6,.01)

hist(re[,2], probability = TRUE, ylim = c(0,0.3), breaks = "scott", main = "A Sample from Gnorm.cdf.upper")
lines(x.seq, dGnorm.upper(x.seq, sdl = sd.intvl.true[1], 
                          sdr = sd.intvl.true[2]), col=2, lty=2)

#mean(re[,2])
#mean(re[,2]^3)

@


<<>>=
hist(re[,1], probability = TRUE, ylim = c(0,0.3), breaks = "scott", main = "A Sample from Gnorm.cdf.lower")
lines(x.seq, dGnorm.lower(x.seq, sdl = sd.intvl.true[1], 
                          sdr = sd.intvl.true[2]), col=2, lty=2)
#we may also involve negative sdl to simplify this simulation 
@



\subsubsection{Estimation using the distributions of the observed data}

One may ask, in this simple setup, since we can explicitly derive
the (marginal) distributions of $\myl{\tilde{\intvl{X}_{t}}}$ and
$\myr{\tilde{\intvl{X}_{t}}}$ based on the property of $\intvl{X}_{t}$,
can we directly use traditional estimation approachs based on the
observed $\myl{\tilde{\intvl{X}_{t}}}$ and $\myr{\tilde{\intvl{X}_{t}}}$? 

To clarify this concern, we will explore this direction here.

If we directly work on the two-end or the center-range quantities
of $\tilde{\intvl{X}}_{t}$, based on their distributions, we have
two directions to do the estimation of $(\mu,\sdl,\sdr)$ (the estimation
of $(\sdl,\sdr)$ is needed to construct confidence interval for $\mu$): 
\begin{enumerate}
\item Use MM (method of moments) or MLE based on the distributions of $[\myl{\tilde{\intvl{X}_{t}}},\myr{\tilde{\intvl{X}_{t}}}]$
to estimate $(\mu,\sdl,\sdr)$; 
\item Use estimate $\mu$ and $(\sdl+\sdr)$ from $\intvlctr{\tilde{\intvl{X}}_{t}}$
and then estimate $(\sdr-\sdl)$ from on $\intvlrng{\tilde{\intvl{X}}_{t}}$
which follows a half normal distribution.
\end{enumerate}

Since the first one involve the moments (which is related to the folded
normal distribution) which is relatively complex in the form (maybe
save it for next week), here we first persue the second direction
(using MM first). Based on the first moment of the half normal distribution,
\[
\lexpt[\intvlrng{\tilde{\intvl{X}}_{t}}]=\sqrt{\frac{2}{\pi}}(\sdr-\sdl),
\]
we have 
\begin{align*}
\hat{\mu}_{\text{cr}}= & \hat{\mu}_{\text{cen}}\coloneqq\frac{1}{n}\sum_{t=1}^{n}\intvlctr{\tilde{\intvl{X}}_{t}},\\
\hat{\sdr}_{\text{cr}} & =(\hat{a}+\hat{b})/2,\\
\hat{\sdl}_{\text{cr}} & =(\hat{a}-\hat{b})/2,
\end{align*}
where 
\begin{align*}
\hat{a} & =2\sqrt{\frac{1}{n-1}\sum_{t=1}^{n}(\intvlctr{\tilde{\intvl{X}}_{t}}-\hat{\mu})^{2}},\\
\hat{b} & =\sqrt{\frac{\pi}{2}}\frac{1}{n}\sum_{t=1}^{n}\intvlrng{\tilde{\intvl{X}}_{t}}.
\end{align*}
The $1-\alpha$ confidence interval for $\mu$ is quite standard,
\[
\hat{\mu}_{\text{cr}}\pm t_{\alpha/2,n-1}\sqrt{\frac{\hat{a}}{2n}}.
\]
We can implement this method under parameter setup: $(\mu,\sdl,\sdr)=(1,1,2)$. 

<<>>=
intvlnormal.ctrrng <- function(intvl.obs, sig.level=0.95){
  rng <- apply(intvl.obs,1,range)
  ctr <- apply(intvl.obs,1,center)
  mu.est <- mean(ctr)
  a.est <- 2*sd(ctr)
  b.est <- sqrt(pi/2)*mean(rng)
  sdr.est <- (a.est+b.est)/2
  sdl.est <- (a.est-b.est)/2
  par.est <- c(mu.est, sdl.est, sdr.est)
  #get the confidence interval
  alpha <- 1-sig.level
  n <- nrow(intvl.obs)
  me <- qt(1-alpha/2, n-1)*sqrt(a.est/(2*n))
  confint.est <- mu.est + c(-1,1)*me
  names(par.est) <- c("mu.est", "sdl.est", "sdr.est")
  re <- list(par.est=par.est, 
             confint.est = confint.est)
  re
}
@

<<>>=
re <- intvlnormal.ctrrng(x.obs)
re
@

Then we can check the sampling distribution. 

<<>>=
#get the sampling distribution
set.seed(123)
#get the sampling distribution
ctrrng.est.mat <- replicate(5e2,{
  x <- rsemiGnorm.intvl(1e3, sd.intvl = c(1,2))+1
  x.obs <- as.propintvl(x)
  intvlnormal.ctrrng(x.obs)$par.est
})
@

<<>>=
hist(ctrrng.est.mat[1,], main = "distn of mu.est using center-range")
abline(v = mu0, col = 2)

hist(ctrrng.est.mat[2,], main = "distn of mu.est using center-range")
abline(v = sd.intvl.true[1], col = 2)

hist(ctrrng.est.mat[3,], main = "distn of mu.est using center-range")
abline(v = sd.intvl.true[2], col = 2)
@

We can also compute the bias and standard error for each estimator by MC. 

<<>>=
compute.bias.se.mc <- function(est.mat, par.true=c(1,1,2)){
  bias.mat <- apply(est.mat, 2, function(x) x-par.true)
  bias.mc <- apply(bias.mat,1,mean)
  se.mc <- apply(bias.mat,1,sd)
  re <- data.frame(Bias=bias.mc, SE=se.mc)
  re
}

(biasse.ctrrng <- compute.bias.se.mc(ctrrng.est.mat))
@



%study the distribution of the two ends

%use interval adjustment 

\subsubsection{Estimations based on the interval adjustment}

Another useful technique we propose here is the so-called \textbf{interval
adjustment}: under the model setup, we want to adjust $\tilde{\intvl{X}}_{t}$
into one appropriate version $\intvl{X}_{t}'$ that can approximately
match the model specification of $\intvl{X}_{t}$. In other words,
adjust the observed dataset to retrieve the underlying interval process
based our current model. The key idea here is the observed $\tilde{\intvl{X}}_{t}$
has the same \textbf{center }as $\intvl{X}_{t}$ which shows the sign
of $\epsilon_{t}$. 

Let 
\[
\tilde{C}_{t}\coloneqq\intvlctr{\tilde{\intvl{X}}_{t}},
\]


Consider 
\[
\hat{\mu}_{\text{cen}}\coloneqq\frac{1}{n}\sum_{t=1}^{n}\tilde{C}_{t}.
\]
Since we have seen that 
\[
\tilde{C}_{t}-\mu=\frac{\sdl+\sdr}{2}\epsilon_{t}.
\]
We can first use ths sign of 
\[
e_{t}=\tilde{C}_{t}-\hat{\mu}_{\text{cen}}
\]
to approximately show the sign of the underlying $\epsilon_{i}$.
Based on the relationship: 
\begin{align*}
\intvlrng{\intvl{X}_{t}} & =(\sdr-\sdl)\epsilon_{t}\\
 & =\abs{(\sdr-\sdl)\epsilon_{t}}\text{sign}(\epsilon_{t})\\
 & =\intvlrng{\tilde{\intvl{X}}_{t}}\text{sign}(\epsilon_{t}).
\end{align*}
We have 
\[
\intvl{X}_{t}=\tilde{\intvl{X}}_{t}\ind{\epsilon_{t}\geq0}+\intvlconj{\tilde{\intvl{X}}_{t}}\ind{\epsilon_{t}<0}.
\]
Since the sign $\epsilon_{t}$ is unknown, we can use the sign of
$e_{t}$ to approximate, to get the adjusted interval 
\[
\intvl{X}_{t}'\coloneqq\tilde{\intvl{X}}_{t}\ind{e_{t}\geq0}+\intvlconj{\tilde{\intvl{X}}_{t}}\ind{e_{t}<0},
\]
as an approximation of $\intvl{X}_{t}$ whose two ends follow normal
distributions. 

Then we can use the estimators, 
\begin{align*}
\hat{\intvl{\mu}}_{\text{adj}} & =[\hat{\mu}_{l},\hat{\mu}_{r}]=\frac{1}{n}\sum_{t=1}^{n}\intvl{X}_{t}',\\
\hat{\sdl}_{\text{adj}} & =\sqrt{\frac{1}{n-1}\sum_{t=1}^{n}(\intvl{X}_{tl}'-\hat{\mu}_{l})^{2}},\\
\hat{\sdr}_{\text{adj}} & =\sqrt{\frac{1}{n-1}\sum_{t=1}^{n}(\intvl{X}_{tr}'-\hat{\mu}_{r})^{2}}.
\end{align*}


To derive the confidence interval, we need to consider the error in
the adjustment step which can be described as, for each fixed $t=1,2,\dotsc,n$,
let $Y_{t}=\tilde{C}_{t}-\mu$, then $Y_{t}\sim N(0,(\sdl+\sdr)^{2}/4)$,
and then $e_{t}=Y_{t}-\bar{Y}_{n}$, 
\begin{align*}
\lprob(\intvl{X}_{t}'=\intvl{X}_{t}) & =\lprob(Y_{t}'=Y_{t})\\
 & =\lprob(e_{t}\epsilon_{t}\geq0)\\
 & =\lprob((Y_{t}-\bar{Y}_{n})Y_{t}\geq0).
\end{align*}

Let $t=1$, since 
\[
\bar{Y}_{n}\convergeto{\lprob}0,
\]
then we can apply the sluskty theorem 
\[
Y_{1}^{2}-\bar{Y}_{n}Y_{1}\convergeto{\text{d}}Y_{1}^{2}.
\]
 

Therefore, 
\[
\lprob(\intvl{X}_{t}'=\intvl{X}_{t})\to\lprob(Y_{1}^{2}\geq0)=1.
\]
We still need to approximate the convergence rate in order to study
the confidence interval rigorously. 

<<>>=
intvlnormal.adj <- function(intvl.obs){
  x.cen <- apply(intvl.obs, 1, center)
  x.cen.bar <- mean(x.cen)
  ind.seq <- as.numeric(x.cen-x.cen.bar<0)
  x.conj <- apply.intvl0(x.obs, intvlconj)
  #adjust.intvl <- function(a) (intvlconj(a)-a)*ind.seq + a
  x.adj <- ind.seq*x.conj + (1-ind.seq)*x.obs
  mean.est <- apply(x.adj, 2, mean)
  mean.est.cen <- mean(x.cen)
  sd.est <- apply(x.adj, 2, sd)
  par.est <- c(mean.est[1], 
              mean.est.cen, 
              mean.est[2], 
              sd.est)
  names(par.est) <- c("meanl.adj", "mean.cen.adj", "meanr.adj", "sdl.est","sdr.est")
  re <- list(x.adj = x.adj, par.est = par.est)
  re
}

re <- intvlnormal.adj(x.obs)
re$par.est
@


<<>>=
x.adj <- re$x.ad
plot.intvl(apply(x.adj,2,cummean))
@

Then we can plot the adjusted intervals and compare it with the true underlying interval process. 
<<>>=
plot.intvl(x.adj[seq_len(1e2), ])
@

<<>>=
plot.intvl(x.obs[seq_len(1e2), ])
@


<<>>=
set.seed(123)
adj.est.mat <- replicate(5e2,{
  x <- rsemiGnorm.intvl(1e3, sd.intvl = c(1,2))+1
  x.obs <- as.propintvl(x)
  par.est <- intvlnormal.adj(x.obs)$par.est
  par.est[c(2,4,5)]
})
@

Check the sampling distribution: 

<<>>=
hist(adj.est.mat[1,], main = "distn of mu.est using adj")
abline(v = mu0, col = 2)

hist(adj.est.mat[2,], main = "distn of sdl.est using adj")
abline(v = sd.intvl.true[1], col = 2)

hist(adj.est.mat[3,], main = "distn of sdr.est using adj")
abline(v = sd.intvl.true[2], col = 2)
@

Check the bias and standard error:

<<>>=
(biasse.adj <- compute.bias.se.mc(adj.est.mat))
@

\subsubsection{Comparison of different estimations methods}

Assessments on this method: 
\begin{enumerate}
\item Estimation method based on center and range is straightforward to
implement and so far it performs better than the adjustment method. 
\item The key benefit of the interval adjustment method is that it can roughly
retrieve the original intervals based on our specified model. It actually
shows the fact that in this context, once we specify a model, we actually
assume a dynamic for the unobserved $\intvl{X}_{t}$. 
\end{enumerate}

More questions to explore: 
\begin{enumerate}
\item How to further improve the center-range estimation method? 
\item What kinds of things may be missed if we only consider the first candidate models?
\end{enumerate}

(One thing is, the performance of one equipment may not be the same as time goes.)


\subsection{One key concern: ambiguity in the interval direction}

The first candidate model is simply a start. As we mentioned before,
the key concern here is we do not know the true $\intvl{X}_{t}$,
therefore any fixed model is actually imposing one possible setup
on the dynamic of $[\myl{\intvl{X}_{t}},\myr{\intvl{X}_{t}}],$ that
is, the probabalistic structure of each end. 

For (P1), if we simply use the traditional methods to do estimation
(which actually does not depend on the direction of the true $\intvl{X}_{t}$),
then we do not need to worry about the interval directions. It actually
shows we are still able to learn the parameters under the ambiguity
in the interval directions.

However, for (P1), if one wants to use the interval adjustment to
check the possible dynamic of $\intvl{X}_{t}$ then use the two ends
of adjusted intervals to construct confidence intervals on $\mu$.
Then the interval directions does matter because we need to consider
the asymptotic behaviour of 
\[
\intvl{Y}_{n}=\frac{1}{\sqrt{n}}\sum_{t=1}^{n}(\intvl{X}_{t}-\mu),
\]
and put the two ends under the $\lexpt[g(\cdot)]$ with $g(x)=\ind{a\leq x\leq b}$.
This is related to (P2): to study the interval expectation of $[\myl{\intvl{Y}_{n}},\myr{\intvl{Y}_{n}}]$
(which are not directly observed): 
\[
[\lexpt[\varphi(\myl{\intvl{Y}_{n}}),\lexpt[\varphi(\myr{\intvl{Y}_{n}}]].
\]


When it comes to (P2), the story has just begun because the dynamic
of $[\myl{\intvl{X}_{t}},\myr{\intvl{X}_{t}}]$ does matter. Previously,
we have specified the first candidate model: 
\begin{align*}
\intvl{X}_{t}^{(1)} & =\mu+[\sdl,\sdr]\epsilon_{t},\\
\tilde{\intvl{X}_{t}} & =1\intvl{X}_{t}^{(1)}.
\end{align*}
Then we have solved (P1) by using two ways to estimate $(\mu,\sdl,\sdr)$.
However, it turns out the underlying process $\intvl{X}_{t}^{(1)}$
is only one possiblity of the true $\intvl{X}_{t}$ such that 
\begin{equation}
\tilde{\intvl{X}_{t}}=1\intvl{X}_{t}.\label{eq:true-to-proper-rel}
\end{equation}
If we consider $\intvl{X}_{t}^{(2)}=\intvlconj{\intvl{X}_{t}^{(1)}}$,
it also statisifies \ref{eq:true-to-proper-rel} but has different
dynamic from $\intvl{X}_{t}$. In other words, if we specify the model
as 
\[
\intvl{X}_{t}^{(2)}=\mu+[\sdr,\sdl]\epsilon_{t},
\]
and if we do the similar estimation procedure (as shown before) based
on the same dataset, we will end up with the same estimation $(\hat{\mu},\hat{\sdl},\hat{\sdr})$
(by rearranging the role of the estimators accordingly), because if
we compare the range in the center-range method, $\abs{(\sdl-\sdr)\epsilon_{t}}$
and $\abs{(\sdr-\sdl)\epsilon_{t}}$ are equal (so they have the same
half-normal distribution). 

One may argue that $\intvl{X}_{t}^{(2)}$ do not have any practical
difference from $\intvl{X}_{t}^{(1)}$ because it simply has opposite
interval direction timewise compared with the other - it provides
the same information on the true underlying process $\intvl{X}_{t}$
if we treat equipment A and B symmetrically. This statement is only
true if the true $\intvl{X}_{t}$ only belongs to one of these two
possible candidate models. 

On the one hand, note that so far $\intvl{X}_{t}^{(i)},i=1,2$ both
assume homoscedasticity for the two ends of $\intvl{X}_{t}$ (they
have constant variance across time). However, this assumption cannot
be validated based on the information provided by the lab. Actually,
in practice, it is highly likely that the equipment has unstable measurement
performance in accuracy across time (due to system deterioration or
other changing environmental factors). 

On the other hand, in our context, since $\intvl{X}_{t}^{(i)},i=1,2$
are both candidate models statisfying \ref{eq:true-to-proper-rel},
if we consider a interval time series switches between $\intvl{X}_{t}^{(1)}$
and $\intvl{X}_{t}^{(2)}$, its proper version will still be $\tilde{\intvl{X}}_{t}$. 

Therefore, we are supposed to at least consider a larger family of
candidate models. In general, if we introduce a discrete-time binary-valued
process $s_{t}:\Omega\to\{0,1\}$ in $(\Omega,\myset{F},(\myset{F}_{t})_{t\geq0},\lprob)$,
\[
\intvl{X}_{t}^{s}\coloneqq\intvl{X}_{t}^{(1)}s_{t}+\intvl{X}_{t}^{(2)}(1-s_{t}),
\]
then at each time $t$, $\intvl{X}_{t}^{s}$ would be either $\intvl{X}_{t}^{(1)}$
or $\intvl{X}_{t}^{(2)}$, so we must have 
\[
\tilde{\intvl{X}_{t}}=1\intvl{X}_{t}^{s},
\]
holds timewise. Meanwhile, we can notice that under this formulation,
\begin{align*}
\intvl{X}_{t}^{s} & =\intvl{X}_{t}^{(1)}s_{t}+\intvl{X}_{t}^{(2)}(1-s_{t})\\
 & =\mu+[(\sdl s_{t}+\sdr(1-s_{t}),(\sdr s_{t}+\sdl(1-s_{t}))]\epsilon_{t}\\
 & \eqqcolon\mu+[\myl{\sigma_{t}},\myr{\sigma_{t}}]\epsilon_{t}.
\end{align*}
It also indicates that the two ends of $\intvl{X}_{t}^{s}$ could
be heteroscedastic (at each $t$, $\myl{\sigma_{t}}$ or $\myr{\sigma_{t}}$
only switches between $\sdl^{2}$ and $\sdr^{2}$, which comes from
the simplified setup of the problem by assuming the constant interval
$[\sdl,\sdr]$; a future extension is to generalize it into $[\sdl_{t},\sdr_{t}]$.) 

Nonetheless, since we only have the observed data $\tilde{\intvl{X}_{t}}$,
it actually does not contain any information on the dynamic of $s_{t}$.
For any two processes $s$ and $s'$ in a set of processes, $\intvl{X}_{t}^{s}$
and $\intvl{X}_{t}^{s'}$ may give different results in (P2) but they
will result in the same observed interval data $\tilde{\intvl{X}_{t}}$:
\[
\tilde{\intvl{X}_{t}}=1\intvl{X}_{t}^{s}=1\intvl{X}_{t}^{s'}.
\]
Since $s$ actually decides the interval direction of $\intvl{X}_{t}$
(either the same as $[\sdl,\sdr]\epsilon_{t}$ or $[\sdr,\sdl]\epsilon_{t}$),
this is called \textbf{the ambiguity in the interval direction}, which
is a true ambiguity if we do not have more background information
on the unobsevable interval $\intvl{X}_{t}$, which does not really
exist in the context of point-valued data because a point, as a degenerate
interval, has no ambiguity in its direction. This leads to one of
the essential differences from the traditional statistical methods
in point-valued data. 

(point-valued data may have distribution or model uncertainty in temporal
sense. It is another kind of ambiguity, which we have studied before.) 

(i.e. any parameters related to the dynamic of $s_{t}$ are not identifiable
based on the data $\tilde{\intvl{X}_{t}}$.)


\subsubsection{Family of Possible Models}

When it comes to the study of the ambiguity (in the interval direction
characterized by $s$), we can no longer only consider a single model
for $s$ but a set $\myset{S}$ of models, each of which will lead
to an interval (linear) expectation concerned by (P2): for any $s\in\myset{S}$,
we have the interval expectation 
\[
\intvlexpt[\intvl{Y}_{n}^{s}]=[\lexpt[\myl{\intvl{Y}_{n}^{s}}],\lexpt[\myr{\intvl{Y}_{n}^{s}}]].
\]
In order to construct an interval that is robust to the choice of
$s$, as an precautious strategy, we can consider 
\[
\hat{\intvlexpt}_{\myset{S}}[\intvl{Y}_{n}]\coloneqq\sup_{s\in\myset{S}}[\lexpt[\myl{\intvl{Y}_{n}^{s}}],\lexpt[\myr{\intvl{Y}_{n}^{s}}]],
\]
where 
\[
\sup_{i\in I}[a_{i},b_{i}]\coloneqq[\inf_{i\in I}a_{i},\sup_{i\in I}b_{i}].
\]
(In this context, the design of the set $\myset{S}$ is decided by
the degree of leakage in the available information on $\intvl{X}_{t}$,
and user may also personally initialize it based on their attitude
or belief and update it once having more information on $\intvl{X}_{t}$.) 

Actually, general picture for the possible specifications of candidate
models: in the classical probability space $(\Omega,\myset{F},(\myset{F}_{t})_{t\geq0},\lprob)$,
let $\epsilon_{t}\sim\text{IID}(0,1)$, consider $s_t:\Omega \to \{0,1\}$ and $s_t$ is (classically) independent from $\epsilon_t$, 
\begin{enumerate}
\item $\intvl{X}_{t}^{(1)}=\mu+[\sdl,\sdr]\epsilon_{t},$
\item $\intvl{X}_{t}^{(2)}=\mu+[\sdr,\sdl]\epsilon_{t}=\intvlconj{\intvl{X}_{t}^{(1)}}$,
\item $\intvl{X}_{t}^{(3)}=\intvl{X}_{t}^{(1)}s_{t}+\intvl{X}_{t}^{(2)}(1-s_{t}),$
where $s_{t}:\Omega\to\{0,1\}$ is any Markov process, 
\item $\intvl{X}_{t}^{(4)}=\intvl{X}_{t}^{(1)}s_{t}+\intvl{X}_{t}^{(2)}(1-s_{t}),$
where $s_{t}:\Omega\to\{0,1\}$ is any $\sigma(s_{t-k},k\geq 1)$-measurable
process,
\item $\intvl{X}_{t}^{(5)}=\intvl{X}_{t}^{(1)}s_{t}+\intvl{X}_{t}^{(2)}(1-s_{t}),$
where $s_{t}:\Omega\to\{0,1\}$ is any $\sigma(s_{t-k},\epsilon_{t-k},k\geq1)$-measurable
process. 
\end{enumerate}
One example for the 3rd case, $s_{t}$ has transition probability,
\[
\left(\begin{array}{cc}
p_{00} & p_{01}\\
p_{10} & p_{11}
\end{array}\right)=\left(\begin{array}{cc}
0.8 & 0.2\\
0.1 & 0.9
\end{array}\right),
\]
where $p_{ij}\coloneqq\lprob(s_{t}=j|s_{t}=i)$ with $i,j=0,1$. Then
in this case, $\intvl{X}_{t}^{(3)}$ can be treated as an interval-valued
hidden Markov model. For the 5th case, 
\begin{align*}
\intvl{X}_{t}^{(5)} & =\intvl{X}_{t}^{(1)}s_{t}+\intvl{X}_{t}^{(2)}(1-s_{t})\\
 & =\mu+[(\sdl s_{t}+\sdr(1-s_{t}),(\sdr s_{t}+\sdl(1-s_{t}))]\epsilon_{t}\\
 & \eqqcolon\mu+[\myl{\sigma_{t}},\myr{\sigma_{t}}]\epsilon_{t}.
\end{align*}
If we let $\myset{F}_{t}\coloneqq\sigma(s_{t-k},\epsilon_{t-k},k\geq0)$,
we can see that 
\[
\myl{\sigma_{t}}:\Omega\to\{\sdl,\sdr\}
\]
 is a $\myset{F}_{t-1}$-measurable process, in other words, a predictable
process; so is $\myr{\sigma_{t}}$ because $\myr{\sigma_{t}}=(\sdl+\sdr)-\myl{\sigma_{t}}$. 

Any more practical examples that we need to consider (P2): 
\begin{enumerate}
\item (in the context of interval-valued log return data)
\end{enumerate}

\subsubsection{Illustration of this ambiguity}

\begin{enumerate}
\item $W_{t}^{(1)}=[\sdl,\sdr]\epsilon_{t}$,
\item $W_{t}^{(2)}=[\sdr,\sdl]\epsilon_{t},$
\item $W_{t}^{(3)}=W_{t}^{(1)}s_{t}+W_{t}^{(2)}(1-s_{t}),$ $s_{t}\sim Bern(1/2)$.
An independent Bernoulli Mixture of version 1 and 2.
\item $W_{t}^{(4)}=W_{t}^{(1)}s_{t}+W_{t}^{(2)}(1-s_{t}),$ $s_{t}$ has
transition probability, 
\[
\left(\begin{array}{cc}
p_{00} & p_{01}\\
p_{10} & p_{11}
\end{array}\right)=\left(\begin{array}{cc}
0.8 & 0.2\\
0.1 & 0.9
\end{array}\right).
\]
A HMM-mixture of version 1 and 2.
\item $W_{t}^{(5)}=W_{t}^{(1)}s_{t}+W_{t}^{(2)}(1-s_{t}),$ $s_{t}=\ind{\epsilon_{t-1}>0}$.
\end{enumerate}

<<fig.width='0.6\\linewidth'>>=
set.seed(1234)
n <- 1e3
sd.intvl.true <- c(1,2)
sig.low <- sd.intvl.true[1]
sig.high <- sd.intvl.true[2]
y.uni <- rnorm(n)
y <- intvl(y.uni, y.uni) #intval
z.prop <- intvl(rep(sig.low,n), rep(sig.high,n))

#noise version 1: semi-G-normal noise
z1 <- z.prop
w1 <- z1*y 

#noise version 2: conjugate of version 1
z2 <- apply.intvl0(z.prop, intvlconj)
w2 <- z2*y

#Independent mixture
#noise version 3: An independent Bernoulli-mixture of version 1 and 2 
s.seq <- rbinom(n, 1, 1/3)
s.mat <- intvl(s.seq, s.seq)
z3 <- z1*s.mat + z2*(1-s.mat)
#z3 <- z1*s.seq + z2*(1-s.seq)
w3 <- z3*y

#plot.intvl(w1, linesplot = TRUE)
#plot.intvl(w2, linesplot = TRUE)
#plot.intvl(w3, linesplot = TRUE)

 
#Dependent mixture
#noise version 4: A HMM-mixture of version 1 and 2
##specify initial state
#interval-data version of one HMM mixture
s0 <- 0 #which is an interval
#s.mat <- matrix(NA, ncol = 2, nrow=n)
s.seq <- numeric(n)
s.seq[1] <- s0
trans.mat <- matrix(c(0.9, 0.1, 
                      0.2, 0.8), 
                    nrow = 2, byrow = TRUE)
for (i in seq_len(n-1)){
  s.seq[i+1] <- as.numeric(s.seq[i]==0)*rbinom(1,1,trans.mat[1,2]) + as.numeric(s.seq[i]==1)*rbinom(1,1,trans.mat[2,2])
}
s.mat <- intvl(s.seq, s.seq)
z4 <- z1*s.mat + z2*(1-s.mat)
#z3 <- z1*s.seq + z2*(1-s.seq)
w4 <- z4*y

#noise version 5: leverage-effect mixture of version 1 and 2
##specify initial state
s0 <- 0 #which is an interval
s.seq <- numeric(n)
s.seq[1] <- s0
#leverage effect
#consider the sign of y.seq[i]

#check the switching rule under varphi(x)=x^3
for (i in seq_len(n-1)){
  #we need to consider the partial sum of w5
  
}

s.seq[-1] <- as.numeric(y.uni[-n]>0)
s.mat <- intvl(s.seq, s.seq)
z5 <- z1*s.mat + z2*(1-s.mat)
#z3 <- z1*s.seq + z2*(1-s.seq)
w5 <- z5*y
 

 
w.list <- list(w1 = w1, 
               w2 = w2,
               w3 = w3, 
               w4 = w4,
               w5 = w5)

#One important note here
#In interval form, they will all be the same in practice (if they all become its proper version)
#their center and range will become the same
#these feature will be all hidden in those proper intervals. 
#this is why we call this phenomenon as the ambiguity in volatility process in the interval-data analysis
#our goal of this experiment is to explicitly show this ambiguity, 
#since we can only observe the proper interval 
#it is nearly impossible to tell which one it is if we are not provided with more reliable information
#so before we  one way to deal with it, is to set up an envelope to have a control on the best and worst case scenario 
#(e.g. to do the estimation)
 

 
w.list.obs <- w.list
sub.ind <- seq_len(1e2)
for (i in seq_along(w.list)){
  w.list.obs[[i]] <- as.propintvl(w.list[[i]])
  plot.intvl(w.list[[i]][sub.ind,], linesplot = TRUE, 
             main = paste0("w.intvl", i))
  plot.intvl(w.list.obs[[i]][sub.ind, ], linesplot = TRUE, 
            main = paste0("w.intvl.obs", i))
}


 
#compare whether their observed versions are the same 
all(w.list.obs[[1]]==w.list.obs[[2]])
all(w.list.obs[[2]]==w.list.obs[[3]])
all(w.list.obs[[3]]==w.list.obs[[4]])
all(w.list.obs[[4]]==w.list.obs[[5]])
 

 
#suppose we assume each kind of underlying model
w.cummean.list <- w.list
for (k in seq_along(w.list)){
  w.cummean.list[[k]] <- apply(w.list[[k]], 2, cummean)
  plot.intvl(w.cummean.list[[k]], linesplot = TRUE)
}
#they all have mean certainty
#mean estimation 
 

 
w.sq.list <- w.sq.cummean.list <- w.list
for (k in seq_along(w.list)){
  w.sq.list[[k]] <- apply.intvl0(w.list[[k]], function(x) x^2)
  #w.sq.list[[k]] <- apply.intvl(w.list[[k]], function(x) x^2)
  w.sq.cummean.list[[k]] <- apply(w.sq.list[[k]], 2, cummean)
  plot.intvl(w.sq.cummean.list[[k]], linesplot = TRUE)
}
#they have the same variance uncertainty in the sense that f(x)=x^2
 

 
w.cb.list <- w.cb.cummean.list <- w.list
for (k in seq_along(w.list)){
  w.cb.list[[k]] <- apply.intvl0(w.list[[k]], function(x) x^3)
  w.cb.cummean.list[[k]] <- apply(w.cb.list[[k]], 2, cummean)
  plot.intvl(w.cb.cummean.list[[k]], linesplot = TRUE)
}
#check the skewness part 
 

 
#suppose we assume one underlying model 

#illustrate how to do the mean estimation in this case
#apply()
#apply the mean function 

#consider the sampling distribution 

#the lack of information on the true underlying process brings us the ambiguity
#how to construct the confidence interval under this ambiguity
#include all possible cases

 


@



Notes: For a directed interval $[a,b]$ with $ab\geq 0$, the sign of the center can show the sign of the interval. The sign of the range can show the direction of the interval. In practice, we can usually observe the sign of the center but not the sign of the range: the sign of the range has not be recorded but only its magnitude. This is one of the reasons we want to point out the ambiguity in the interval direction. 

\subsubsection{Use $G$-expectaiton to deal with this Ambiguity}
In (P2), suppose $\mu=0$, and consider the normalized sum of the
true interval process $\intvl{X}_{t}$ (with unknown interval direction),
\begin{align*}
\intvl{Y}_{n} & =\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\intvl{X}_{t}=\frac{1}{\sqrt{n}}\sum_{t=1}^{n}[(\sdl s_{t}+\sdr(1-s_{t}),(\sdr s_{t}+\sdl(1-s_{t}))]\epsilon_{t}\\
 & =[\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\myl{\sigma_{t}}\epsilon_{t},\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\myr{\sigma_{t}}\epsilon_{t}].
\end{align*}
We are interested in
\begin{align*}
\intvlexpt[\intvl{Y}_{n}\varphi] & =\intvlexpt[\varphi(\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\myl{\sigma_{t}}\epsilon_{t}),\varphi(\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\myr{\sigma_{t}}\epsilon_{t})]\\
 & =[\lexpt[\varphi(\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\myl{\sigma_{t}}\epsilon_{t})],\lexpt[\varphi(\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\myr{\sigma_{t}}\epsilon_{t})]].
\end{align*}
In many cases, these two quantities are related, 
\begin{align*}
\intvlexpt[\varphi(\intvl{Y}_{n})] & =\intvlexpt[\varphi\intvl{Y}_{n}]\\
 & =\intvlexpt[1(\intvl{Y}_{n}\varphi)]\\
 & =\intvlexpt[(\intvl{Y}_{n}\varphi)\intvlcup(\intvlconj{\intvl{Y}_{n}\varphi})].
\end{align*}


The key problem here is, the interval expectation we are interested
in does depend on the dynamic of $s_{t}$ which we cannot learn from
the observed data $\tilde{\intvl{X}}_{t}=1\intvl{X}_{t}$. However,
we are able to learn $(\sdl,\sdr)$ from the data $\tilde{\intvl{X}_{t}}$.
Before we have more information on the true underlying interval process
$\intvl{X}_{t}$, we can consider all possible reasonable dynamic
for $s_{t}$ and create an convex hull as the envelope to have a control
on the best and worst case scenario for a given $\varphi$. Recall
the notations: for two directed intervals $\intvl{x},\intvl{y}\in\intvlspace{\numset{R}}$
\begin{align*}
\sup\{\intvl{x},\intvl{y}\} & \coloneqq\intvl{x}\intvlcup\intvl{y}\\
 & =[\inf\{\myl{x},\myl{y}\},\sup\{\myr{x},\myr{y}\}].
\end{align*}
Consider all possible dynamic for $s_{t}$: (double check the terminology)
let $\myset{S}$ represent the set of all predictable processes valuing
in $\{0,1\}$. 
\[
\myset{S}=\{s_{t}:\Omega\to\{0,1\},\myset{F}_{t-1}\text{-measurable},t=0,1,2,\dotsc,n\}
\]
Then 
\begin{align*}
\hat{\intvlexpt}_\myset{S}[\intvl{Y}_{n}\varphi] & \coloneqq\sup_{s\in\myset{S}}\intvlexpt[\intvl{Y}_{n}^{s}\varphi]=\intvlcup_{s\in\myset{S}}\intvlexpt[\intvl{Y}_{n}^{s}\varphi]\\
 & =[\inf_{s\in\myset{S}}\lexpt[\varphi(\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\myl{\sigma_{t}}^{s}\epsilon_{t})],\sup_{s\in\myset{S}}\lexpt[\varphi(\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\myl{\sigma_{t}}^{s}\epsilon_{t})]]\\
 & \to[-\expt[-\varphi(W)],\expt[\varphi(W)]]=[-\GN_{G}[-\varphi],\GN_{G}[\varphi]].
\end{align*}
where $W\sim\GN(0,\varI)$. 

%(This part may be also related to the CUSUM procedure in interval data context.)

This part will be especially useful in the study of interval-valued
log return dataset. For instance, similar to the spirit in $G$-VaR,
if we consider $\varphi(x)=\ind{X\leq a}$, then the results above
can be rewritten as the statement that
\begin{align*}
[\inf_{s\in\myset{S}}\lprob(\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\myl{\sigma_{t}}^{s}\epsilon_{t} & \leq a),\sup_{s\in\myset{S}}\lprob(\frac{1}{\sqrt{n}}\sum_{t=1}^{n}\myl{\sigma_{t}}^{s}\epsilon_{t}\leq a)]\\
\to & [\lowprob(W\leq a),\upprob(W\leq a)]\\
= & [\mylower{F}_{\GN}(a),\myupper{F}_{\GN}(a)].
\end{align*}

%compare their performance 

%the key part is the confidence interval

%ambiguity in the interval direction

\subsection{Mean Uncertainty and Variance Uncertainty}

The first candidate model is, with $0<\sdl<\sdr$, and $\meanl<\meanr$, 

\[
\intvl{X}_{t}=\intvl{X}_{t}^{(1)}=[\meanl,\meanr]+[\sdl,\sdr]\epsilon_{t},
\]
as the underlying process and $\tilde{X}_{t}=1\intvl{X}_{t}.$ Suppose
$\epsilon_{t}\sim\CN(0,1)$, then we have the two-end representation
of $\intvl{X}_{t}$, 

\[
\begin{cases}
\myl{\intvl{X}_{t}}=\meanl+\sdl\epsilon_{t} & \text{i.i.d.}\sim\CN(\meanl,\sdl^{2})\\
\myr{\intvl{X}_{t}}=\meanr+\sdr\epsilon_{t} & \text{i.i.d.}\sim\CN(\meanr,\sdr^{2})
\end{cases}.
\]
The center-range representation of $\intvl{X}_{t}$ are, 
\[
\begin{cases}
\intvlctr{\intvl{X}_{t}}=\frac{\meanl+\meanr}{2}+\frac{\sdl+\sdr}{2}\epsilon_{t} & \text{i.i.d.}\sim\CN(\frac{\meanl+\meanr}{2},\frac{(\sdl+\sdr)^{2}}{4})\\
\intvlrng{\intvl{X}_{t}}=\meanr-\meanl+(\sdr-\sdl)\epsilon_{t} & \text{i.i.d.}\sim\CN(\meanr-\meanl,(\sdr-\sdl)^{2})
\end{cases}.
\]
Accordingly, we can write the two-end representation of $\tilde{\intvl{X}}_{t}$,
\[
\myl{\tilde{\intvl{X}_{t}}}=(\meanl+\sdl\epsilon_{t})\wedge(\meanr+\sdr\epsilon_{t}),
\]
and similarly, 

Consider the center-range representation 
\[
\intvlctr{\tilde{\intvl{X}}_{t}}=\intvlctr{\intvl{X}_{t}}=\frac{\meanl+\meanr}{2}+\frac{\sdl+\sdr}{2}\epsilon_{t}\sim\CN(\frac{\meanl+\meanr}{2},\frac{(\sdl+\sdr)^{2}}{4}),
\]
and 
\[
\intvlrng{\tilde{\intvl{X}}_{t}}=\abs{\intvlrng{\intvl{X}_{t}}}=\abs{(\meanr-\meanl)+(\sdr-\sdl)\epsilon_{t}}.
\]
\begin{align*}
\intvlrad{\tilde{\intvl{X}_{t}}} & =\frac{1}{2}\intvlrng{\tilde{\intvl{X}}_{t}}=\abs{\frac{\meanr-\meanl}{2}+\frac{\sdr-\sdl}{2}\epsilon_{t}}\\
 & \sim\CN_{f}(\frac{\meanr-\meanl}{2},\frac{(\sdr-\sdl)^{2}}{4}).
\end{align*}
Folded normal distribution: $Y\sim\CN_{f}(\mu,\sigma^{2})$ if $Y=|X|$
with $X\sim\CN(\mu,\sigma^{2})$, note that we assume $\mu>0$ here
to ensure the identifiability of the parameter (otherwise, $(\mu,\sigma^{2})$
and $(-\mu,\sigma^{2})$ will result in the same density.) 

Let 
\begin{align*}
\mu_{C} & \coloneqq\frac{\meanl+\meanr}{2},\\
\sigma_{C} & \coloneqq\frac{\abs{\sdl+\sdr}}{2},
\end{align*}
and 
\begin{align*}
\mu_{r} & \coloneqq\frac{\abs{\meanr-\meanl}}{2}=\frac{\meanr-\meanl}{2},\\
\sigma_{r} & \coloneqq\frac{\abs{\sdr-\sdl}}{2}.
\end{align*}
Under the assumption $\sdl\leq\sdr$ and $\abs{\sdl}\leq\abs{\sdr}$,
if $\sdr<0$, then 
\[
\sdl\leq\sdr<0,
\]
it is contradicts to $\abs{\sdl}\leq\abs{\sdr}$. Then $\sdr\geq0$.
Hence 
\[
\sdr=\abs{\sdr}\geq\abs{\sdl}\geq-\sdl,
\]
namely, 
\[
\sdr+\sdl\geq0.
\]
Meanwhile, we obviously have $\sdr-\sdr\geq0$. Therefore, we must
have 
\[
\sigma_{C}=\frac{\sdl+\sdr}{2},
\]
and 
\[
\sigma_{r}=\frac{\sdr-\sdl}{2}.
\]
There is a one-to-one correspondence between $(\meanl,\meanr,\sdl,\sdr)$
and $(\mu_{C,}\mu_{r},\sigma_{C},\sigma_{r})$:
\[
\begin{cases}
\meanl & =\mu_{C}-\mu_{r}\\
\meanr & =\mu_{C}+\mu_{r}\\
\sdl & =\sigma_{C}-\sigma_{r}\\
\sdr & =\sigma_{C}+\sigma_{r}
\end{cases}.
\]
In matrix form, 
\[
\mymat{A}=\mymat{K}\tilde{\mymat{A}},
\]
where 
\[
\mymat{K}\coloneqq\left(\begin{array}{cc}
1 & -1\\
1 & 1
\end{array}\right),
\]
 
\[
\mymat{A}=(\begin{array}{cc}
\myvec{\mu} & \myvec{\sigma}\end{array})\coloneqq\left(\begin{array}{cc}
\meanl & \sdl\\
\meanr & \sdr
\end{array}\right),
\]
and 
\[
\tilde{\mymat{A}}=(\begin{array}{cc}
\tilde{\myvec{\mu}} & \tilde{\myvec{\sigma}}\end{array})\coloneqq\left(\begin{array}{cc}
\mu_{C} & \sigma_{C}\\
\mu_{r} & \sigma_{r}
\end{array}\right).
\]
Then we can use estimation equations or directly use MLE to estimate
$\tilde{\mymat{A}}$ first, then use $\tilde{\mymat{A}}$ to estimate
$\mymat{A}$.

\subsubsection{Ambiguity in the interval direction}

We have $\tilde{\intvl{X}_{t}}=1\intvl{X}_{t}^{(i)}$
, $i=1,2,3,4$ where
\begin{enumerate}
\item $\intvl{X}_{t}^{(1)}=[\meanl,\meanr]+[\sdl,\sdr]\epsilon_{t}$, with
$\meanl\leq\meanr$, $\sdl\leq\sdr$ and $\abs{\sdl}\leq\abs{\sdr}$, 
\item $\intvl{X}_{t}^{(2)}\coloneqq\intvlconj{\intvl{X}_{t}^{(1)}}=[\meanr,\meanl]+[\sdr,\sdl]\epsilon_{t}$, 
\item $\intvl{X}_{t}^{(3)}=[\meanl,\meanr]+[\sdr',\sdl']\epsilon_{t}$,
with $\meanl\leq\meanr$, $\sdl'\leq\sdr'$ and $\abs{\sdl'}\leq\abs{\sdr'}$,
\item $\intvl{X}_{t}^{(4)}\coloneqq\intvlconj{\intvl{X}_{t}^{(3)}}=[\meanr,\meanl]+[\sdl',\sdr']\epsilon_{t},$
\end{enumerate}
(Draft version) Consider $s_{t}:\Omega\to\{1,2,3,4\}$ (independent
from $\epsilon_{t}$) 
\[
\intvl{X}_{t}^{s}=\sum_{i=1}^{4}\intvl{X}_{t}^{(i)}\ind{s_{t}=i}.
\]
$s_{t}$ could belong to 
\begin{enumerate}
\item $\myset{M}$: the set of all Markov processes ($s_{t}$ is independent
from $\epsilon_{t}$), this belongs to HMM-mixture model, 
\item $\myset{S}_{0}$: the set of all $\sigma(s_{t-k},k\geq0)$-measurable
process, 
\item $\myset{S}$: the set of all $\sigma(s_{t-k},k\geq0,\epsilon_{t-l},l\geq1)$-measurable
process. 
\end{enumerate}
We have 
\[
\myset{M}\subset\myset{S}_{0}\subset\myset{S}.
\]
For the last one, if we let $\myset{F}_{t}\coloneqq\sigma(s_{t-k},\epsilon_{t-k},k\geq0)$,
then we have $s_{t}$ is $\myset{F}_{t}$-measurable (also note that
$s_{t}$ is independent from $\epsilon_{t}$).

any Markov process $\subset$ $\sigma(s_{t-k},k\geq0)$-measurable
$\subset$ $\sigma(s_{t-k},k\geq0,\epsilon_{t-l},l\geq1)$-measurable)
process.

The limiting theorems to talk about ambiguity in this situation is still under discussions. Part of the newest results are available in the Ref:G-limit-thm. 

<<>>=
#set.seed(1234)
n <- 1e3
sd.intvl.true <- c(-2,3)
mean.intvl.true <- c(0,1)
w <- rsemiGnorm.intvl(n, sd.intvl = sd.intvl.true)
w.obs <- as.propintvl(w)
mu0 <- rconstant.intvl(n, mean.intvl.true)
x <- mu0 + w
x.obs <- as.propintvl(x)
 

 
sub.ind <- seq_len(2e2)
plot.intvl(w[sub.ind,])
plot.intvl(w.obs[sub.ind,])
plot.intvl(x[sub.ind,])
plot.intvl(x.obs[sub.ind,])
#how to do the adjustment to do the estimation
 

 
hist(x.obs[,1])
hist(x.obs[,2])
 

 
x.cen <- apply(x.obs, 1, center)
x.rad <- apply(x.obs, 1, radius)
x.rng <- apply(x.obs, 1, range)

hist(x.cen)
hist(x.rad)
hist(x.rng)


@

<<eval=FALSE>>=
n <- 2e3
M <- 50
set.seed(1234)
est.mat <- replicate(M,{
w <- rsemiGnorm.intvl(n, sd.intvl = sd.intvl.true)
w.obs <- as.propintvl(w)
mu0 <- rconstant.intvl(n, mean.intvl.true)
x <- mu0 + w
x.obs <- as.propintvl(x)
est <- intvlnormal.ctrrng(x.obs, meanCertainty = FALSE)
est
})
#error message:Error in while (sum(abs(anew - aold)) > tol) { : 
#  missing value where TRUE/FALSE needed
#Called from: Rfast::foldnorm.mle(rad, tol = 1e-10)
#
@

<<eval=FALSE>>=
par.true <- c(mean.intvl.true, sd.intvl.true)
(main.seq <- rownames(est.mat))
for (i in seq_len(nrow(est.mat))){
  hist(est.mat[i,], main = paste("Sampling Distribution of", main.seq[i]), breaks = "scott")
  abline(v=par.true[i], col=2, lty=2)
}
@


\section{Real Data Examples}

\subsection{The Big Picture}

Consider a stochastic process $X_{t}$ with $t\in\myset{T}$ and $\myset{T}$
is an arbitrary index set. 
\begin{enumerate}
\item If $\myset{T}=\numset{N}\coloneqq\{0,1,2,\dotsc\}$, $X_{t}$ is a
discrete-time process; 
\item If $\myset{T}=[0,+\infty]$, $X_{t}$ is a continuous-time process. 
\end{enumerate}
How should we understand the possible Knightian uncertainty or Ambiguity
characterized by $\myset{Q}\coloneqq\{F_{\theta}\}_{\theta\in\Theta}$
for $X_{t}$? 
\begin{enumerate}
\item (point-valued data) $X_{t}\sim F_{\theta_{t}}\in\myset{Q}$ and $\theta_{t}$
may switch in $\Theta$ as $t$ increases (in an unknown way). If
user already has a model as a filter, $X_{t}$ may play a role as
the (varying) parameter in the model or the filtered noise part. 
\item (interval-valued data) $\myset{X}_{t}=\{X_{t}^{\theta}\sim F_{\theta_{t}},\theta_{t}\in\Theta\text{ or }X_{t}^{\theta}\sim F_{\theta},\theta\in\Theta\}$,
but we can only observe $1\intvl{X}_{t}=\myset{X}_{t}^{h}$. (Here
we also consider the ambiguity in the interval direction.) 
\end{enumerate}

\subsection{Objectives}

How should we view the interval-valued daily log return dataset? 

Two objectives: 
\begin{enumerate}
\item (O1) Find a model that can mimic the data pattern of the interval-valued
log return (to do forecasting), 
\item (O2) We want to study the expected daily return of an agent with some
trading strategy, which is related to the ambiguity in the interval
direction. (Our current focus)
\end{enumerate}
For (O1), we can try following methods. 

\subsection{Interval-valued log return}

We will study the S&P500 index (symbol:\^GSPC). 

<<>>=
library(quantmod)

#Real Data preparation
startdate <- "2000-01-01" #avoid the holidays (a minor issue here)
#enddate <- "2019-01-01"
enddate <- "2020-05-24" #avoid the holidays

#try higher frequency to have more points
#North American Market: S&P500
#getSymbols("^GSPC", from=startdate, to=enddate, by=60*60)
getSymbols("^GSPC", from=startdate, to=enddate)

head(GSPC)

time.ind <- "2010/2020"
S.intvl.xts <- GSPC[time.ind, c("GSPC.Low", "GSPC.High")]

S.intvl <- intvl(left = as.numeric(S.intvl.xts$GSPC.Low), 
                 right = as.numeric(S.intvl.xts$GSPC.High)) 

#S.intvl <- intvl(left = )

#without as.numeric, S.intvl is auotmatic a xts object
#xts object has the name issue, there is conflict in the change of name if apply it to the data frame
#plot.intvl(S.intvl)

plot(S.intvl.xts)


#compute the return sequence 
#directly work on the xts object
n.t <- nrow(S.intvl.xts)
t.ind <- seq_len(n.t-1) + 1
S.low <- as.numeric(S.intvl.xts$GSPC.Low)
S.high <- as.numeric(S.intvl.xts$GSPC.High)

#xts object at different time point cannot directly do the difference

#relative return 
return.seq.high <- (S.high[-1] - S.low[-n.t])/S.low[-n.t]
return.seq.low <- (S.low[-1] - S.high[-n.t])/S.high[-n.t]

#return.seq.high <- (S.high[-1] - S.low[-n.t]) #absolute return
#return.seq.low <- (S.low[-1] - S.high[-n.t]) #absolute return

# return.seq.low <- return.seq.high <- S.high[-1]
# 
# for (t in t.ind){
#   return.seq.low[t-1] <- S.high[t]-S.low[t-1]
#   return.seq.low[t-1] <- S.low[t]-S.high[t-1]
# }



return.intvl.xts.org <- xts(cbind(return.seq.low, return.seq.high), order.by = index(S.intvl.xts[-1,]))
return.intvl.xts <- return.intvl.xts.org*100
colnames(return.intvl.xts) <- c("return.low", "return.high")



plot(return.intvl.xts)
#this is a typical interval type data.
#we definitely need to adapt a G-version time series/stochastic model to describe this pattern. 
#e.g. G-GARCH model with noise part itself has variance uncertainty.
 

#return.center.xts <- apply(return.intvl.xts, 1, mean)
return.center.xts <- (return.intvl.xts[,1]+return.intvl.xts[,2])/2
#why its center is always greater than zero. 
return.radius.xts <- (return.intvl.xts[,2]-return.intvl.xts[,1])/2

plot(return.center.xts)
plot(return.radius.xts)
#it definitely have an interval-type noise part
#we need to consider the model uncertainty in this case
#G-ARCH model in this case
#current literature is mostly modeling the center part
#how to understand the range part
#it may be another sign to show the volatility status of the market

hist(return.center.xts, breaks = "scott")
hist(return.radius.xts, breaks = "scott")
@

Check the data patterns. 

<<>>=
#check the correlation between left and right end
return.low.num <- as.numeric(return.intvl.xts[,1])
return.high.num <- as.numeric(return.intvl.xts[,2])

plot(return.low.num, return.high.num)
abline(a=0,b=1,col=2)

plot(abs(return.low.num), abs(return.high.num))
abline(a=0,b=1,col=2)

plot(log(abs(return.low.num)), log(abs(return.high.num)))
abline(a=0,b=1,col=2)
@

We can also study the ratio: abs(center)/radius. 

<<>>=
return.center.num <- as.numeric(return.center.xts)
return.radius.num <- as.numeric(return.radius.xts)

ratio1 <- return.center.xts/return.radius.xts 
ratio2 <- abs(return.center.xts)/return.radius.xts

plot(ratio1)
#hline(c(-1,1), col=2)

plot(ratio2)

#abline(h=1, col=2)

hist(ratio2)
abline(v=1, col=2)

#abline(a=0,b=1,col=2)
plot(return.center.num, return.radius.num)

plot(abs(return.center.num), return.radius.num)

mod <- lm(return.radius.num~abs(return.center.num))

summary(mod)
@


\subsection{Construction from a Finer Time Grid: Point-valued Data}


\subsubsection{A Single Stochastic Volatility Model}

(As suggested by Prof. Kulperger) Since the the interval form of the
log return data itself comes from the lowest and highest value of
the intraday stock price, we can consider a finer time grid and stochastic
volatility: $W_{t}$ is the classical Brownion motion in $(\Omega,\myset{F},\mathbb{F}=(\myset{F}_{t})_{t\geq0},\lprob)$.
Let $\myset{F}_{t}\coloneqq\sigma(W_{s},s\leq t)$. We can assume
a reasonable model for $\sigma_{t}$ (any $\myset{F}_{t}$-measurable
process). 
\[
\frac{\diff S_{t}}{S_{t}}=\sigma_{t}\diff W_{t},
\]
namely, 
\[
\diff\log S_{t}=\sigma_{t}\diff W_{t},
\]
or in integral form from time $0$ to time $t$, 
\[
\log S_{t}=\log S_{0}+\int_{0}^{t}\sigma_{s}\diff W_{s},
\]
or, the log return from $t$ to $t+\Delta t$ (e.g. $\Delta t$ is
one day): 
\[
\log S_{t+\Delta t}-\log S_{t}=\int_{t}^{t+\Delta t}\sigma_{s}\diff W_{s}.
\]
In order to consider the interval-valued log return, we first need
to have the lowest and highest stock price. Set up the larger time
grid (for daily values) as: $t_{i}=t_{0}+i\Delta t$. 
\[
\mylower{lS}_{i}\coloneqq\min_{t_{i}\leq t<t_{i+1}}\log S_{t}=\log S_{0}+\min_{t_{i}\leq t<t_{i+1}}\int_{0}^{t}\sigma_{s}\diff W_{s}
\]
and 
\[
\myupper{lS}_{i}\coloneqq\max_{t_{i}\leq t<t_{i+1}}\log S_{t}.
\]
We have the daily return (based on the closing price) as 
\[
R_{i}\coloneqq\log S_{t_{i}}-\log S_{t_{i-1}}=\int_{t_{i-1}}^{t_{i}}\sigma_{s}\diff W_{s}.
\]
Then we also have the observed interval-valued log return $[\mylower{R}_{i},\myupper{R}_{i}]$
as 
\[
\mylower{R}_{i}=\mylower{lS}_{i}-\myupper{lS}_{i-1},
\]
and 
\[
\myupper{R}_{i}=\myupper{lS}_{i}-\mylower{lS}_{i-1}.
\]
We must have $R_{i}\in[\mylower{R}_{i},\myupper{R}_{i}]$. For a given
function (as a test/loss/gain/utitility function), we can consider
\[
[\lexpt[\varphi(\mylower{R}_{i})],\lexpt[\varphi(\myupper{R}_{i})].
\]
We also have 
\[
(\lexpt[\varphi(\log\frac{S_{t_{i}}}{S_{t_{i-1}}})]=)\lexpt[\varphi(R_{i})]=\lexpt[\varphi(\int_{t_{i-1}}^{t_{i}}\sigma_{s}\diff W_{s})].
\]

Next we can implement a simulation study by specifying $\sigma_t$ as a GARCH(1,1) model. Let $r_t = \log R_t$. Assume
\[
r_t = \sigma_t \epsilon_t,
\]
and
\[
\sigma_t^2 = \alpha+\alpha_1 r^2_{t-1} + \beta_1 \sigma_{t-1}^2.
\]
Here we set $(\alpha, \alpha_1, \beta_1)=(1e-5, 0.08, 0.9).$

<<>>=
###Simulation study
###GARCH(1,1) on a finer time grid 
rgarch11 <- function(n, par){
  a0 <- par[1]
  a1 <- par[2]
  b1 <- par[3]
  ep <- rnorm(n)
  sig2 <- x <- numeric(n)
  sig2[1] <- a0/(1-(a1+b1))
  x[1] <- sqrt(sig2[1])*ep[1]
  for (i in seq_len(n-1)){
    sig2[i+1] <- a0 + a1*x[i]^2 + b1*sig2[i]
    x[i+1] <- sqrt(sig2[i])*ep[i]
  }
  list(x.seq=x, sig2.seq=sig2)
}

re <- rgarch11(5e4, c(1e-5, 0.08, 0.9))

#use burn in sample 

plot(re$x.seq, type = "l")
hist(re$x.seq, breaks = "scott")
plot(re$sig2.seq, type = "l")

library(moments)
#kurtosis(re$x.seq)

#try min and max

x.seq <- re$x.seq
S.seq <- 1e3+cumsum(x.seq)
#plot(S.seq, type = "l")
S.mat <- matrix(S.seq, ncol = 100, byrow = TRUE)
S.intvl0 <- apply(S.mat, 1, function(x) c(min(x), max(x)))
S.intvl <- mat2intvl(t(S.intvl0))


#try the simulation study 
get.return.intvl <- function(S.intvl){
  n.t <- nrow(S.intvl)
  S.low <- S.intvl[,1]
  S.high <- S.intvl[,2]
  return.seq.high <- (S.high[-1] - S.low[-n.t])/S.low[-n.t]
  return.seq.low <- (S.low[-1] - S.high[-n.t])/S.high[-n.t]
  return.intvl <- intvl(left = return.seq.low, right = return.seq.high)
  return.intvl
}
return.intvl <- get.return.intvl(S.intvl = S.intvl)

plot.intvl(S.intvl)
plot.intvl(return.intvl)

return.intvl.cen <- apply(return.intvl,1,center)
return.intvl.rad <- apply(return.intvl,1,radius)

hist(return.intvl.cen*100)
kurtosis(return.intvl.cen)

plot(abs(return.intvl.cen), return.intvl.rad)
abline(a=0, b=1, col=2)

ratio2.1 <- abs(return.intvl.cen)/return.intvl.rad
plot(ratio2.1, type = "l")

hist(ratio2.1)

#it may be due to volatility clustering 

#check this direction 

#to do the simulation study

#then we step into the traditional procedure to mimic the behaviour of the dataset

@

Then the next question is how to do the estimation or calibration
in this case. Before that, how should we study the property of $[\mylower{lS}_{i},\myupper{lS}_{i}]$. 

This could be one possible future direction for us to imagine and
study. Since $\log S_{t}$ is a martingale, one general result in
classical stochastic analysis so far we can apply here is the Martingale
inequality, which is an extension of Kolmogorov's maximum inequality,
let 
\[
X_{t}\coloneqq\log S_{t}.
\]
Let 
\[
M_{u}=\max_{t\leq u}X_{t},
\]
and 
\[
m_{u}=\min_{t\leq u}X_{t}.
\]
Then 
\[
\myupper{lS}_{i}\coloneqq\max_{t_{i}\leq t<t_{i+1}}X_{t}=X_{t_{i}}+\max_{t_{i}\leq t<t_{i+1}}(X_{t}-X_{t_{i}}).
\]


Since both $-X_{t}$ and $X_{t}$ are submartingale, we can apply
the martingale inequalities (Bernstein and Lévy). For any $r\geq0$
and $u\in[0,\infty)$, since $X_{t}$ is a submartingale, we have
\begin{align*}
r\lprob(M_{u}\geq r) & \leq\lexpt[X_{u}\ind{M_{u}\geq r}]\leq\lexpt[X_{u}^{+}],\\
r\lprob(m_{u}\leq-r) & \leq-\lexpt[X_{0}]+\lexpt[X_{t}\ind{m_{u}>-r}\leq\expt[X_{u}^{+}]-\lexpt[X_{0}].
\end{align*}
We can also apply these inequalities to $-X_{t}$. If we have a parametric
form for $\sigma_{t}$, we can derive the distribution of $[\mylower{lS}_{i},\myupper{lS}_{i}]$. 


\subsubsection{Study of Uncertainty in the Volatility Model}

The ideas here will come back to our previous discussions in the model
uncertainty for point-valued data. 

If we specify a certain parametric model for the dynamic of $\sigma_{t}$
and we calibrate the model based on the current dataset, as time goes
and new data come in, in practice, it is required to re-calibrate
the model and update the parameters. This essentially indicates we
cannot ignore the time-varying feature of the parameter for a given
model specification. (It is usually hard to absorb the dynamic of
this change into our model to achieve the persistency of the parameter:
a classical regime-switching model will be one candidate but its transition
probability matrix cannot be ensured that there is no need to be updated
after some time. We simply do not know how the future dataset will
behave. We are discussing a fundamental concern of the model specification
here.)

Furthermore, in different periods, the underlying model structure
(i.e. the parametric form) itself may also change: for example, the
normal versus crisis period of a financial market. (This is also one
of the motivations of the Bayesian pooling or other more sophisicated
hierarchical models to impose different model structures on different
periods and there is a regime-switching structure assumed.) 

We want to start from the other direction to think about these concerns:
what is the largest set of possibilities for $\sigma_{t}$ (driven
by only endogenous dynamic)? 

(If we are uncertain about the dynamic of $\sigma_{t}$, then the
first question in this point-valued data situation is, how uncertain
we are?) 

We are still in this log return data example: 
\[
\log S_{t}=\log S_{0}+\int_{0}^{t}\sigma_{t}\diff W_{t}.
\]


If we consider the volatility ambiguity/uncertainty, that is, the
ambiguity in the dynamic of $\sigma_{t}$, which can be characterized
by 
\[
\myset{A}\coloneqq\{\sigma_{t}:\text{ any }\myset{F}_{t}\text{-measurable process valuing in }\sdInt\}.
\]
This is indeed a really large set, we can update it once we have more
information on the datasets (e.g. if the user want to match the stylized
patterns of the dataset, we can shrink this set.) A smaller set: 
\[
\myset{A}_{0}=\{\sigma_{t}:\text{any }\sigma(\sigma_{s},s\leq t)\text{-measurable process valuing in }\sdInt\}.
\]
 If we only consider the point-valued daily return (based on the closing
price), for any possible dynamic for $\sigma_{t}$, we have the associated
log return $R_{i}^{\sigma}$. 

Then we can consider the extreme cases for $\lexpt[\varphi(R_{i})]$
(this is still for point-valued data), 
\begin{align*}
\sup_{\sigma\in\myset{A}}\lexpt[\varphi(R_{i}^{\sigma})] & =\sup_{\sigma\in\myset{A}}\lexpt[\varphi(\int_{t_{i-1}}^{t_{i}}\sigma_{s}\diff W_{s})]\\
 & =\expt[\varphi(B_{t_{i}}-B_{t_{i-1}})]\\
 & =\expt[\varphi(\sqrt{t_{i}-t_{i-1}}B_{1})],
\end{align*}
where $B_{1}\sim\GN(0,\varI)$. $B_{t}$ is the $G$-Brownian motion
in $\exptSpace$.

Furthermore, from (ref: Denis-Hu-Peng) we have 
\begin{align*}
\sup_{\sigma\in\myset{A}}[\varphi(R_{1},R_{2},\dotsc,R_{n})] & =\sup_{\sigma\in\myset{A}}\lexpt[\varphi(\int_{t_{i-1}}^{t_{i}}\sigma_{s}\diff W_{s},i=1,2,\dotsc,n)]\\
 & =\expt[\varphi(B_{t_{1}},B_{t_{2}}-B_{t_{1}},\dotsc,B_{t_{n}}-B_{t_{n-1}})].
\end{align*}

(The relation with the interval data)
Since one agent may get a daily return $R_i \in [\mylower{R}_{i},\myupper{R}_{i}]$ depending her trading strategy which can achieve outcome that can be approximated by one $\sigma$. (it does make sense that, if we assume the expected value of log return is persistently zero, we can assume any trading strategy which could not see into the future is also expected to have zero return.)

For given $W_t$, we can assume that 
\[[\mylower{R}_{i},\myupper{R}_{i}] = \{\int_{t_{i-1}}^{t_{i}}\sigma_{s}\diff W_{s}, \sigma \in \myset{A}\}.
\]
(It may be better to consider the bid-ask price.)

(We can also consider any element $\sigma$, then we can see that $\lexpt[\varphi(\sum_i R_i)]$ is sufficiently covered by the G-expectation of the G-BM, which is a strict subset of the expectation of $[\sum_i \mylower{R}_{i},\sum_i \myupper{R}_{i}]$.

%In this way, we have, 
%Note that
%this supreme does not have a direct relation with the interval
%data $[\mylower{R}_{i},\myupper{R}_{i}]$. 

Readers may ask, when will we consider this kind of large set in theory
or practice? From the knowledge of authors, here are several possibilities: 
\begin{enumerate}
\item Consider a general statistical inference for the structure of $\sigma_{t}$:
\[
H_{0}:\sigma\in\myset{A}_{0},\textbf{ vs }H_{a}:\sigma\in\myset{A}/\myset{A}_{0}.
\]

\item Provide a scheme to check the capability of a general volatility model
which have large flexibility in its structure (e.g. the neuron stochastic
volatility model): for a given $\varphi$, we can use the nonlinear
expectation to provide the theoretical maximum of 
\[
\expt[\varphi(\sum_{i}\sigma_{i}W_{i})],
\]
and also provide the argmax for this extreme case (the optimal $\sigma_{t}$),
use it as a way to check the capability of a neuron network (to check
whether it can learn the optimal $\sigma_{t}$ to achieve the extreme
case.) 
\end{enumerate}
If we are uncertain about the dynamic of $\sigma_{t}$, what is the
envelope to cover the possible values of $\lexpt[\varphi(R_{i})]$
for a given $\varphi$? 


\subsection{Next development}
\begin{enumerate}
\item We may also consider other distributions for $\epsilon_{t}$ or only
consider it as $\text{IID}(0,1)$. 
\item Mean uncertainty (or the mean certainty is unknown) and variance uncertainty:
first candidate model 
\[
\intvl{X}_{t}=[\meanl,\meanr]+[\sdl,\sdr]\epsilon_{t}.
\]

\item A direct extension based on this example is the simple linear regression: \end{enumerate}



\end{document}
